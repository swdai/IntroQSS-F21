---
title: 7.1 Statistical Inference
subtitle: PLSC30500, Fall 2021
author: 
  # - co-taught by Molly Offer-Westort & Andy Eggers
  - .small-text[(This lecture with references to Aronow & Miller 2019)]
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: no
---

```{r setup, include=FALSE}
library(tidyverse)
set.seed(60637)
options(width = 60)
```

```{css, echo=FALSE}
.small-output .remark-code{
  font-size: x-small;
}

# .show-only-last-code-result pre + pre:not(:last-of-type) code[class="remark-code"] {
#     display: none;
# }
```

# Introduction to estimation


Returning to our example where we flip a coin twice, let $X$ be the number of heads we observe. Our coin is *not* fair, and the probability of getting a heads is 0.8. 

--

The random variable's probability distribution is then:

$$
f(x) = \begin{cases}
1/16 & x = 0 \\\
3/8 & x = 1 \\\
9/16 & x = 2 \\\
0 & \text{otherwise}
\end{cases}
$$

---

Let's take a look at the mean. 

```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}
plotdata <- tibble(
  x = c(0, 1, 2),
  xend = c(1, 2, 3),
  fx = c(1/16, 3/8, 9/16),
  Fx = cumsum(fx)
)

# Expected value
Ex <- sum(plotdata$x*plotdata$fx)

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +  
  geom_vline(xintercept = 1.5, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=1.5, y=0.75, label="E[X]") +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
```

--

$$
\begin{align}
\textrm{E}[X] & = \sum_x x fx \\\\
& = 0 \times \frac{1}{16} + 1 \times \frac{3}{8} + 2 \times \frac{9}{16}\\\
& = \frac{24}{16}\\\
& = 1.5
\end{align}
$$

---

And the spread.  

```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +  
  geom_vline(xintercept = Ex, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=Ex, y=0.75, label="E[X]", color = 'grey') +
  geom_segment(aes(x = Ex, xend = Ex+sign(x-Ex)*(x-Ex)^2, y = fx, yend = fx), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  annotate(geom="text", x=1.8, y=0.63, label="Squared distance\nfrom mean") +
  annotate(geom="text", x=(plotdata$x+Ex)/2, y=(plotdata$fx-0.05), label=(plotdata$x-Ex)^2, color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
```

Variance = average squared distance from the mean
$$
\begin{align}
\textrm{Var}[X] & = \textrm{E}[(X - \textrm{E}[X)^2]\\\\
& = 2.25 \times \frac{1}{16} + 0.25 \times \frac{3}{8} + 0.25 \times \frac{9}{16} \\\\
& = 0.375
\end{align}
$$


---
And the spread.  

```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}

sdx <- sqrt(sum((plotdata$x-Ex)^2 *plotdata$fx))

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +  
  geom_vline(xintercept = Ex, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=Ex, y=0.75, label="E[X]", color = 'grey') +
  geom_segment(aes(x = Ex, xend = Ex-sdx, y = 0.5, yend = 0.5), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  geom_segment(aes(x = Ex, xend = Ex+sdx, y = 0.5, yend = 0.5), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  annotate(geom="text", x=1.8, y=0.63, label="Square root of average\nsquared distance\nfrom mean") +
  annotate(geom="text", x=(Ex+c(-1.05,1.05)*round(sdx, 3)/2), y=0.45, 
           label=round(sdx, 3), color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
```

SD = square root of variance
$$
= \sqrt{0.375} = 0.612
$$

---


We can check our calculations of the expectation and spread in R. 

--

First, we'll want to simulate the random process. 

```{r}
n <- 1000
X <- c(0, 1, 2)
probs <- c(1/16, 3/8, 9/16)
x_observed <- sample(X, prob = probs, replace = TRUE, size = n)

head(x_observed)
```

--


```{r}
mean(x_observed)
```

--

```{r}
var(x_observed)
sd(x_observed)
```

---

The process that we just did -- sampling and estimation based on observed data -- is a very common process in empirical research. 

--

But we may notice that the mean, variance, and standard deviation are not exactly what we calculated mathematically. 

---

Let's try it again. 

--


```{r}
x_observed <- sample(X, prob = probs, replace = TRUE, size = n)

mean(x_observed)
var(x_observed)
sd(x_observed)
```

---

The values that we get are close, but not identical. 

--

This is because what we are observing in practice is a *sample* from the data. 


---

# Sampling and statistics

Very often, we only observe a limited number of observations, which are drawn from a larger population. 

--

We can summarize the data we observe with *statistics*. Statistics are functions of the data we observe.
--


$$
T_n = h(X_1, \dots, X_n)
$$

--

This should look a lot like our definiton of *estimators*. Estimators are a class of statistics that we use to approximate specific estimands.

--

Because our sampling process is a random process, these statistics themselves are random variables, with their own distributions. 

--

We can describe our sample, but we might also like to *make inferences* about the larger population--i.e., to summarize what we know about that population based on the data we observe. 

--
This is what we use statistics for, and why we talk about probability AND statistics. 

--

Probability gives us a model of the world. 

--
Statistics give us a way to relate the data that we see to the model. 

---

In our two coin flip example, suppose we don't know whether the coin is fair or not. We can observe the results of a large number of coin flips, and make an educated guess about the underlying population value. 

--

Formally, that educated guess is called *estimation*. 

---

## i.i.d. Data

The standard treatment of estimation in statistics is built around the assumption that observations are *independent and identically distributed*. 

--

Formally, if we have $n$ draws, $X_1, \dots, X_n$, these draws are i.i.d. if they are independent from each other, and all have the same CDF. 

$$
X_1, \dots, X_n \sim F_X
$$

*Notational aside: $~$ is read as "distributed," and means that the random variable $X$ has the distribution function $F$.*

---

In our coin flip example, each time we flip the coin:
- what we see on this flip has no relation to previous or future flips
- the distribution of heads on every flip is identical. 

--

Our coin flip random process produces i.i.d. random variables. 

---



## Sample mean

Let's repeat our random sampling from the double coin flip, but we'll consider a smaller sample, of size $n = 100.$

```{r}
n <- 100
x_observed <- sample(X, prob = probs, replace = TRUE, size = n)

head(x_observed)
```

--

Our *sample mean* is the mean we observe in our data.
--
This is one of the most commonly used sample statistics. It's called a plug-in estimator, because we just "plug in" the sample analog of the population quantity that we're interested in. 

--

$$\bar{X}_n = \frac{X_{1} + \dots + X_n}{n} = \frac{1}{n} \sum_{i = 1}^n X_i$$





--

```{r}
mean(x_observed)
```


---


We differentiate the *sample mean* from the *population mean* because the sample mean will vary with every new sample we draw. 

--

We'll use a `prrr::map` to see what would happen if we took a sample of size $n = 100$ from the population distribution many times. 

```{r}

n_iter <- 10000

x_list <- map(1:n_iter, ~ sample(X, prob = probs, replace = TRUE, 
                                 size = n))

head(x_list, 2)
```

---
```{r, message=FALSE}

sample_means <- map(x_list, mean)
head(sample_means)
```

---

```{r, message=FALSE}

df_sm <- tibble(sample_means = unlist(sample_means))
df_sm
```

---

```{r fig.width = 6, fig.height=4, fig.align='center'}
ggplot(df_sm, aes(x = sample_means)) +
  geom_histogram(bins = 20, position = 'identity', color = 'white') +
  geom_vline(xintercept = Ex, color = 'grey', lty = 'dashed')

```


--

We see the sample means are roughly distributed around the mean of the underlying population, `r Ex`. 

--

The expected value of the sample mean is the population mean. 


---

We can check that this is the case. 


$$
\begin{aligned}
\bar{X}\_n & = \frac{X\_{1} + \dots + X\_n}{n} = \frac{1}{n} \sum\_{i = 1}^n X\_i
\end{aligned}
$$

--


By linearity of expectations $\dots$

$$
\begin{aligned}
\textrm{E}[\bar{X}\_n] & = \frac{1}{n} \sum_{i = 1}^n \textrm{E}[X_i]\\
\end{aligned}
$$

--

$\dots$ because the $X_i$ are i.i.d. $\dots$

$$
\begin{aligned}
& = \frac{1}{n} \sum_{i = 1}^n \textrm{E}[X]
\end{aligned}
$$

--

$$
\begin{aligned}
& = \frac{1}{n} n \textrm{E}[X]\\
\end{aligned}
$$

--

$$
\begin{aligned}
& = \textrm{E}[X]
\end{aligned}
$$


--

*Refer to Aronow & Miller p.97 for more extended version of proof*. 

---
class: small-output 

## Sample variance

We can estimate the mean of the population using the sample mean. What about the sample variance? 

--

We'll do the same process with our simulations.

--

```{r fig.width = 6, fig.height=4, fig.align='center'}
sample_var <- map(x_list, var)
df_sv <- tibble(sample_var = unlist(sample_var))

ggplot(df_sv, aes(x = sample_var)) +
  geom_histogram(bins = 20, position = 'identity', color = 'white') +
  geom_vline(xintercept = sdx^2, color = 'grey', lty = 'dashed')
```


--

We see the sample variances are roughly distributed around the variance of the underlying population, `r sdx^2`. 

---


The formula for the unbiased sample variance is: 

$$S^2_n = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)^2$$
--

This looks a little bit different from a straightforward sample analog to the population variance formula, 

$$
\textrm{Var}[X] = \textrm{E}[(X-\textrm{E}[X]^2]
$$
--

Why do we divide by $n-1$, instead of $n$?

---

$$S^2_n = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)^2$$

The sample mean, $\bar{X}_n$, has an expected value of $\textrm{E}[X]$.

--

However, because it is made up of the $1, \dots, n$ $X_i$ that we actually observe, the expected difference between $(X_i - \bar{X}_n)$ is a little bit smaller than the expected difference between $(X_i - \textrm{E}[X])$. 


--

To account for this, we divide by $n-1$, instead of $n$. 

---
class: tiny

## 

We can check that the expected value of the unbiased sample variance estimator is the population variance. 
--
The estimator:

--

$$
\begin{aligned}
S^2\_n = \frac{1}{n-1}\sum\_{i = 1}^n (X\_i - \bar{X}\_n)^2
\end{aligned}
$$


--
Take the expectation:

$$
\begin{aligned}
\textrm{E}[S^2\_n] & = \textrm{E}\left[\frac{1}{n-1}\sum\_{i = 1}^n (X\_i - \bar{X}\_n)^2\right]
\end{aligned}
$$


--
By linearity of expectations:

$$
\begin{aligned}
 & = \frac{1}{n-1}\sum\_{i = 1}^n \textrm{E}\left[(X\_i - \bar{X}\_n)^2\right]
\end{aligned}
$$


--


$$
\begin{aligned}
& = \frac{1}{n-1}\sum\_{i= 1}^n \textrm{E}\left[X\_i^2 -2 X\_i\bar{X}\_n + \bar{X}\_n^2\right]
\end{aligned}
$$

---

<!-- -- -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -2\textrm{E}[ X\_i\frac{1}{n}\sum\_{j = 1}^n X\_j] + \textrm{E}\left[ \left(\frac{1}{n}\sum\_{j = 1}^n X_j \right) ^2\right]\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- --- -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -\frac{2}{n}\left(\textrm{E}[X\_i^2] +  \textrm{E}\left[\sum\_{j \neq i} X\_i X\_j\right]\right) + \textrm{E}\left[ \left(\frac{1}{n}\sum\_{j = 1}^n X_j \right) ^2\right]\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- -- -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -\frac{2}{n}\left(\textrm{E}[X\_i^2] +  \sum\_{j \neq i} \textrm{E}\left[ X\_i X\_j\right]\right) + \frac{1}{n^2} \textrm{E}\left[\sum\_{j = 1}^nX\_j^2 + \sum\_{j = 1}^n \sum\_{k \neq j}^nX\_j X\_k  \right]\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- --- -->

Expand expressions, and use linearity of expectations again. 

$$
\begin{aligned}
 & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -\frac{2}{n}\left(\textrm{E}[X\_i^2] +  \sum\_{j \neq i} \textrm{E}\left[ X\_i X\_j\right]\right) + \\\\
 \frac{1}{n^2} \left( \sum\_{j = 1}^n\textrm{E}[X\_j^2] + \sum\_{j = 1}^n \sum\_{k \neq j}^n \textrm{E}[ X\_j X\_k]  \right)\right)
\end{aligned}
$$

--

Because our data is i.i.d.:

$$
\begin{aligned}
 & = \frac{n}{n-1} \left( \textrm{E}[X^2] -\frac{2}{n}\left(\textrm{E}[X^2] +  (n-1) \textrm{E}[ X]^2\right) + \frac{1}{n^2} \left(n\textrm{E}[X^2] + n(n-1) \textrm{E}[ X]^2 \right) \right)
\end{aligned}
$$

<!-- -- -->



<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{n}{n-1} \left( \frac{n - 2 + 1}{n} \textrm{E}[X^2] + \frac{-2(n-1) + (n-1)}{n} \textrm{E}[ X]^2 \right) -->
<!-- \end{aligned} -->
<!-- $$ -->

--

Organizing terms:

$$
\begin{aligned}
 & = \frac{n}{n-1} \left( \frac{n - 1}{n} \textrm{E}[X^2] - \frac{n-1 }{n} \textrm{E}[ X]^2 \right)
\end{aligned}
$$

--

And simplifying: 

$$
\begin{aligned}
 & = \textrm{E}[X^2] - \textrm{E}[ X]^2 
\end{aligned}
$$

---

We can check to see how R calculates in the `var()` function. 

--

```{r}
head(x_observed)
```

--

```{r}
var(x_observed)
```

--

```{r}

mean( (x_observed - mean(x_observed) )^2)*n/(n-1)

```

--

R uses the formula for the unbiased sample variance. 







---

The sample mean is itself a random variable, and so it has its own mean and variance. The mean of the sample mean is the population mean. The variance of the sample mean is:

$$
\textrm{Var}[\bar{X}] = \frac{\textrm{Var}[X]}{n}
$$


--

Let's check this in our simulation as well. We saw that mathematically, $\textrm{Var}[X]$ was 0.375. So
$$
\frac{\textrm{Var}[X]}{n} = \frac{0.375}{100} = 0.00375
$$

--

```{r}
var(unlist(sample_means))
```

--

It's not exactly what we calculated mathematically. 

---

In fact, from our `r n_iter` separate samples, we calculate `r n_iter` separate sample means. From the variation in these sample means, we again *estimate* the variance of the sample mean. 

--

But *this estimate is itself a random variable*, with, again, its own sampling distribution. We will get slightly different estimates of the sampling variance of the sample mean each time we take our `r n_iter` separate samples. 

---
# Weak Law of Large Numbers



As our sample size $n$ grows, we are increasingly likely to observe a sample mean $\bar X_n$ that is close to the mean of the distribution, $\textrm{E}[X]$

--

Formally, 

If $X_1, \dots, X_n$ are i.i.d. random variables,  then 

$$
\bar{X}_n \overset{p}{\to} \textrm{E}[X].
$$

--
Convergence in probability means that the probability that we measure a value of $\bar X_n$ that is any arbitrary distance away from $\textrm{E}[X]$ is decreasing with our sample size. 



---
class: small-output

## 

To see this, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE }
n <- 500
x_list <- map(1:8, ~ sample(X, prob = probs, replace = TRUE, 
                                 size = n))

x_mat <- as_tibble(x_list, .name_repair = 'unique')

new_mat <- x_mat %>% 
  mutate(across(everything(), ~ cumsum(.x)/seq_along(.x)),
         n = 1:n) %>% 
  pivot_longer(cols = starts_with('..'),
               values_to = "Sample mean")

ggplot(new_mat, aes(x = n, y = `Sample mean`, col = name)) +
  geom_hline(yintercept = Ex, color = 'darkgrey', lty = 'dashed') +
  geom_line() +
  theme(legend.position = 'none')
  

```

---
Why is the WLLN so helpful to us?

- connect to finite sampling, infinite super populations


---

# De moivre la place theorem


