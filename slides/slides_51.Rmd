---
title: "PLSC30500, Fall 2021"
subtitle: "5.1 Regression: introduction"
# author: "Andy Eggers & Molly Offer-Westort"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: false

---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align = "center", out.width = "900px", fig.width = 4.5, fig.asp = .7)
```



class: inverse, middle, center

# Quick follow-up on problem set

---

```{r}
set.seed(30500)
n <- 1000
dat <- tibble(Y0 = runif(n = n, min = 0, max = 1)) %>% 
  mutate(Y1 = Y0,
         D = rbinom(n, 1, Y1),
         D_random = rbinom(n, 1, .5))
dat
```

---

## Decomposition of difference in means 

The difference in means can be decomposed into 

$$\text{Diff-in-means} = \text{ATT} + \text{Selection bias}$$ 

where 

$$\text{ATT} = E[Y_i(1) | D_i = 1 ] - E[Y_i(0) | D_i = 1 ]$$ 

and 

$$\text{Selection bias} = E[Y_i(0) | D_i = 1 ] - E[Y_i(0) | D_i = 0 ]$$

(Show on board.)

--

Given random assignment, $(Y_i(1), Y_i(0)) \perp\!\!\!\!\perp D_i$, which implies $Y_i(0) \perp\!\!\!\!\perp D_i$, which means 

$$E[Y_i(0) | D_i = 1 ] = E[Y_i(0) | D_i = 0 ] = E[Y_i(0)]$$

which means no selection bias.


---


class: inverse, middle, center

# Regression basics: what it is, why we do it

---

## Regression: what is it for?

In **regression analysis**, we estimate the relationship between a **dependent variable** $(Y_i)$ and **independent variables** $(X_i, D_i, \ldots)$

<br> </br>

Regression **describes** relationships in a dataset, but in some circumstances we can use it for  

- prediction (what will $Y_i$ be, given some $X_i$?)
- causal inference (what is effect of $D_i$ on $Y_i$?)

---

## Ordinary least squares regression

When we say "regression" we often mean **ordinary least squares (OLS)**.

--

Suppose we want to describe $Y_i$ as a function of $D_i$ and $X_i$ in some data: 

$$ \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i $$ 

Note: 

- the " $\hat{}$ "s ("hat"s) indicate estimates; without the " $\hat{}$ " they are population parameters ("truth")
- $\hat{\beta}_0$, $\hat{\beta}_1$, $\hat{\beta}_2$ are "coefficients" or "coefficient estimates"   

--

Define $Y_i - \hat{Y_i} = \hat{r}_i$ as the **residual**. 

--

We seek coefficient estimates that minimize the **sum of squared residuals**, i.e.

$$\underset{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2}{\arg\min} \sum_{i=1}^n  \left(Y_i - (\hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i)\right)^2$$

<!-- Don't know why the math doesn't work. I want \sum_{i=1}^n -->

---

## Toy example 1


.pull-left[

Suppose we have this data: 
```{r get-data, echo = F}
dat <- tribble(~x, ~y,
        0, -1,
        1, 1,
        1, 2)

dat %>% 
  kableExtra::kbl(full_width = F, digits = 2)
```
]

.pull-right[
```{r, out.width = "500px", echo = F}
dat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  expand_limits(x = c(-.25, 1.25), y = c(-1.25, 2.25))

```
]

Regression equation: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$

<br> </br>


What **intercept** $\hat{\beta}_0$ and **slope** $\hat{\beta}_1$ would minimize the sum of squared residuals? 

What is the minimum sum of squared residuals?

???

At intercept of -1 and slope of 2.5, the SSR is $.5^2 + .5^2 = .5$ 

---

## Toy example 1

```{r}
# base R
lm(y ~ x, data = dat)
# estimatr package
estimatr::lm_robust(y ~ x, data = dat)
```

---

## Toy example 1

```{r, out.width="70%"}
dat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_smooth(method = lm, se = F) + 
  geom_point()
```

---


## Toy example 2

Suppose we have this data: 
```{r get-data-2, out.width = "60%", echo = F}
set.seed(1245)
n <- 100
dat2 <- tibble(x = rnorm(n), 
               y = x + rnorm(n, sd = .8))

dat2 %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(size = 2) +
  coord_fixed()
```

<br> </br>

What **intercept** $\hat{\beta}_0$ and **slope** $\hat{\beta}_1$ would minimize the sum of squared residuals? 

---

## Toy example 2

```{r}
# base R
lm(y ~ x, data = dat2)
# estimatr package
estimatr::lm_robust(y ~ x, data = dat2)
```

---

## Toy example 2

```{r, out.width="70%"}
dat2 %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_smooth(method = lm, se = F) + 
  geom_point() + 
  coord_fixed()
```



---

## How does `R` minimize the sum of squared residuals?

Calculus and linear algebra. 

<br> </br>
<br> </br>

(See Linear Models, next course in sequence!)

---

## *Why* does OLS minimize the sum of squared residuals?

Correct but superficial answers: 

- "squared" to make residuals positive
- "squared" because sum of absolute residuals wouldn't always give a unique answer
- "squared" to make the math prettier/easier than e.g. absolute residuals

--

Deeper answers:

- minimizing the sum of squared residuals gives the **conditional mean** or a linear approximation to it; other approaches (e.g. mean absolute residual) do not
- suppose we want a predictor that (in the population) minimizes mean squared error (MSE), i.e. $E[(Y - \hat{Y})^2]$; then the corresponding "plug-in estimator" minimizes the sum of squared residuals (see e.g. Aronow & Miller)
- the predictor that minimizes MSE in the population is the **conditional expectation function (CEF)** $E[Y | X]$; OLS delivers a linear approximation to the CEF 

---

class: bg-full
background-image: url("assets/MSE_SSR_diagram_3.png")
background-position: center
background-size: 75%

## Why sum of squared residuals? 

---

class: inverse, middle, center

# Regression with categorial variables: a (clumsy) way to recover group means

---

## Butler & Broockman (2011) again

```{r}
bb <- read_csv('../data/legislators_email/Butler_Broockman_AJPS_2011_public_csv.csv')

bb %>% 
  group_by(treat_deshawn) %>% 
  summarize(mean(reply_atall))

lm(reply_atall ~ treat_deshawn, data = bb)

```
How are these related?

---

## From regression output to conditional means 

```{r}
lm(reply_atall ~ treat_deshawn, data = bb)
```

--

The **fitted regression equation** can be written 

$$ \hat{Y_i} = .574 - .018 D_i $$ 
--

So the prediction (conditional mean) depends on the value of $D_i$: 


$$
\hat{Y} = \begin{cases}
.574 & D_i = 0 \\\
.574 - .018 & D_i = 1 
\end{cases}
$$
---

## Regressing on factor variables 

```{r}
bb2 <- bb %>% 
  mutate(treatment = ifelse(treat_deshawn == 1, "deshawn", "jake"))
lm(reply_atall ~ treatment, data = bb2)
```

--

Key concept: **omitted category**. When we regress on a factor or character variable, `R` designates an omitted category and reports a coefficient for all other categories.

---

## Omitted category: another example

```{r}
bb %>% 
  count(leg_party)
lm(reply_atall ~ leg_party, data = bb)
```


---

## Another way

Instead of estimating 

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 \mathrm{DeShawn}_i$$
(with emails saying "Jake Mueller" as the omitted group), we can estimate 

$$\hat{Y}_i = \hat{\alpha}_1 \mathrm{NotDeShawn}_i + \hat{\alpha}_2 \mathrm{Deshawn}_i$$
by adding a `-1` to the formula syntax:

```{r}
lm(reply_atall ~ factor(treat_deshawn) - 1, data = bb)
```

---

## Another example

```{r}
lm(reply_atall ~ leg_party - 1, data = bb)
```

---

## Adding a predictor

`leg_republican` indicates whether the email recipient was a Republican.

```{r}
lm(reply_atall ~ treat_deshawn + leg_republican, data = bb)
```

What is the predicted probability of a reply for a "Deshawn" email to a Republican?

--

```{r echo = F}
reg1 <- lm(reply_atall ~ treat_deshawn + leg_republican, data = bb)
coefs <- coef(reg1) %>% round(3)
```

The prediction is $`r coefs[1]` + `r coefs[2]` + `r coefs[3]` = `r sum(coefs)`$.

---

## Adding a predictor (2)

```{r}
lm(reply_atall ~ treat_deshawn + leg_republican, data = bb) %>% 
  coef()
```

If we write the prediction equation as  $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 \mathrm{DeShawn}_i + \hat{\beta}_2 \mathrm{LegRep}_i$, then we have

| Email | Recipient | Predicted response rate | 
|-------|:---------:|:-----------------------:|
| Jake  |   Dem     |      $\hat{\beta}_0$    |
| DeShawn  |   Dem     |      $\hat{\beta}_0 + \hat{\beta}_1$   |
| Jake  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_2$    |
| DeShawn  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_2$    |

--

So what is the predicted DeShawn-vs-Jake difference for Democrats? For Republicans? 

---

## Adding an interaction 

<!-- To avoid imposing the assumption that Democrats and Republicans respond the same way to DeShawn-vs-Jake, we add an **interaction**. -->

```{r}
lm(reply_atall ~ treat_deshawn*leg_republican, data = bb) %>% 
  coef()
```

The prediction equation becomes 

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 \mathrm{DeShawn}_i + \hat{\beta}_2 \mathrm{LegRep}_i + \hat{\beta}_3 \mathrm{DeShawn}_i  \times \mathrm{LegRep}_i$$
so we have

| Email | Recipient | Predicted response rate | 
|-------|:---------:|:-----------------------:|
| Jake  |   Dem     |      $\hat{\beta}_0$    |
| DeShawn  |   Dem     |      $\hat{\beta}_0 + \hat{\beta}_1$   |
| Jake  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_2$    |
| DeShawn  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_2 + \hat{\beta}_3$    |

---

## Interpreting interactions 

Compare: 

```{r}
lm(reply_atall ~ treat_deshawn*leg_republican, data = bb) %>% 
  coef()
```

```{r}
bb %>% 
  group_by(leg_republican, treat_deshawn) %>% 
  summarize(resp_rate = mean(reply_atall))
```


---

## Summing up

- in a regression using only categorical explanatory variables, the predictions are just sample means for categories
- to recover the predictions/means, you usually have to add up coefficients: this can be tricky

---

class: inverse, middle, center

# Regression with continuous predictors: non-linear predictions (and over-fitting) with linear regression

---


## US presidential election data

Here is a dataset on U.S. presidential elections from 1948 to 2016: 

```{r}
pres <- read_csv("./../data/pres_data.csv") %>% 
  filter(year < 2020)
```

| Variable Name | Description | 
|-------:|:-----------------------:|
|`year` | Election year |
| `deminc` | 1=incumbent is a Democrat |
| `incvote` | Percent of the two-party vote for the incumbent party |
|`q2gdp` | Second-quarter change in real GDP in election year |
| `juneapp` | Net approval of incumbent president in June of election year |

---

## Incumbent approval rating and incumbent party vote share 

```{r, out.width="70%", echo = F}
vote_app <- pres %>% 
  ggplot(aes(x = juneapp, y = incvote, label = year)) + 
  geom_point() + 
  geom_text(nudge_x = 2, nudge_y = .5, size = 2) 
vote_app
```

---

## Coefficient interpretation 

.pull-left[
```{r, echo = F}
vote_app + 
  geom_smooth(method = lm, se = F)
```
]

.pull-right[
```{r}
lm(incvote ~ juneapp, 
   data = pres)
```
]

How do we explain/interpret the coefficient on `juneapp`? 

--

Be explicit about units & meaning of variables; avoid causal language unless justified (e.g. by randomization). 

- .red[**Bad:**] "The effect of `juneapp` on `incvote` is .165"
- .light-green[**Good:**] "A one percentage point increase in presidential approval is associated with a .16 percentage point increase in votes for the incumbent party" **or** "The predicted vote for the incumbent party goes up by .16 percentage points when the approval rating goes up by one percentage point"


---

# Standard `R` regression output

```{r}
lm(incvote ~ juneapp, data = pres) # minimal
```

---

# Standard `R` regression output

```{r}
lm(incvote ~ juneapp, data = pres) %>% 
  summary()  # more extensive
```


---

# Standard `R` regression output (`estimatr`)

```{r}
estimatr::lm_robust(incvote ~ juneapp, data = pres) # minimal
```

---

# Standard `R` regression output (`estimatr`)

```{r}
estimatr::lm_robust(incvote ~ juneapp, data = pres) %>% 
  summary() # more extensive
```

---

## $R^2$

$R^2$ measures the proportion of variance in the dependent variable "explained" by the independent variables.

--

**In-sample** measure of **goodness-of-fit** (vs **out-of-sample** measures like *cross-validation*).

--

If $\hat{y}_i$ is the prediction (fitted value) for unit $i$, then 

$$R^2 = \frac{\mathrm{var}(\hat{y}_i)}{\mathrm{var}(y_i)}$$

#### Alternative formulation


$$\mathrm{Sum\ of\ squared\ residuals}: \qquad \mathrm{SS}_{\mathrm{res}} = \sum_i (y_i - \hat{y}_i)^2$$

$$\mathrm{Total\ sum\ of\ squares}: \qquad \mathrm{SS}_{\mathrm{tot}} = \sum_i (y_i - \overline{y})^2$$

$$R^2 = 1 - \frac{\mathrm{SS}_{\mathrm{res}}}{\mathrm{SS}_{\mathrm{tot}}}$$
---

## $R^2$ by hand

```{r}
mod <- lm(incvote ~ juneapp, data = pres)
names(mod)
names(summary(mod))
summary(mod)$r.squared
```

--

```{r}
var(mod$fitted.values)/var(pres$incvote)
```

---

## $R^2$ by hand (tidier)

```{r}
augmented <- lm(incvote ~ juneapp, data = pres) %>% 
  broom::augment() %>% 
  select(incvote, .fitted) 

augmented %>% head()

augmented %>% 
  summarize(var_fitted = var(.fitted),
            var_y = var(incvote)) %>% 
  mutate(r2 = var_fitted/var_y)
```

---

## $R^2$: what is it good for?

Usually reported but not discussed.

Can be useful for comparing across models of the same outcome.

--

Not a good way of assessing whether someone's research is successful, because 

- if causal inference is our goal, low $R^2$ could mean treatment effect small relative to other factors
- if causal explanation is our goal, correlation is not causation 
- if prediction is our goal, good **in-sample** fit necessary but not sufficient for good *out-of-sample* fit

--

A key challenge is *over-fitting*.

---

## Modeling non-linearity

What would be a good predictive model for this (fake) dataset?

```{r, out.width = "70%", echo = T}
dat <- tibble(x = rnorm(1000, sd = 1.25),
       y = .25*(x^3 + 2*x^2 - 4*x - 6) + rnorm(1000, sd = 2)) 
p <- dat %>% 
  ggplot(aes(x, y)) + 
  geom_point(size = 1, alpha = .5)
p
```

---

## Modeling non-linearity

```{r, out.width = "80%", out.height="50%"}
p + 
  geom_smooth(method = lm, se = F) + 
  labs(title = "Linear fit",
       subtitle = str_c("R^2 = ", 
        summary(lm(y ~ x,data = dat))$r.squared %>% round(2)))
```

Regression equation: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$

"First-degree polynomial" (linear)

---

## Modeling non-linearity

```{r, out.width = "80%", out.height="50%"}
p + 
  geom_smooth(method = lm, formula = y ~ x + I(x^2), se = F) + #<< 
  labs(title = "Quadratic fit",
       subtitle = str_c("R^2 = ", 
        summary(lm(y ~ x + I(x^2), data = dat))$r.squared %>% round(2)))
```

Regression equation: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{\beta}_2 X_i^2$

"Second-degree polynomial" (quadratic)
---

## Modeling non-linearity


```{r, out.width = "80%", out.height="50%"}
p + 
  geom_smooth(method = lm, formula = y ~ x + I(x^2) + I(x^3), se = F) + #<<  
  labs(title = "3rd degree polynomial fit",
       subtitle = str_c("R^2 = ", 
          summary(lm(y ~ x + I(x^2) + I(x^3), data = dat))$r.squared %>% round(2)))
```

Regression equation: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{\beta}_2 X_i^2 + \hat{\beta}_3 X_i^3$

---

## Modeling non-linearity w. `poly`


```{r, out.width = "80%", out.height="50%"}
p + 
  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), se = F) +  #<<
  labs(title = "3rd degree polynomial fit",
       subtitle = str_c("R^2 = ", summary(lm(y ~ poly(x, 3), data = dat))$r.squared %>% round(2)))
```


---

## Modeling non-linearity w. `poly`

```{r, out.width = "80%", out.height="50%"}
p + 
  geom_smooth(method = lm, formula = y ~ poly(x, 7), se = F) + 
  labs(title = "7-th degree polynomial fit",
       subtitle = str_c("R^2 = ", summary(lm(y ~ poly(x, 7), data = dat))$r.squared %>% round(2)))
```

---

## Beware over-fitting! 

With 18 data points, a 7th- or 15th-degree polynomial can (over)fit very well! 

```{r, echo = F, out.width = "80%", out.height="50%"}
pres %>% 
  ggplot(aes(x = juneapp, y = incvote)) +
  geom_point() + 
  geom_smooth(method = lm, se = F) + 
  labs(title = "Linear fit", 
       subtitle = str_c("R^2 = ", summary(lm(incvote ~ juneapp, data = pres))$r.squared %>% round(2))) + 
  coord_cartesian(ylim = range(pres$incvote) + c(-10, 10))
```

---

## Beware over-fitting! 

With 18 data points, a 7th- or 15th-degree polynomial can (over)fit very well! 

```{r, echo= F, out.width = "80%", out.height="50%"}
pres %>% 
  ggplot(aes(x = juneapp, y = incvote)) +
  geom_point() + 
  geom_smooth(method = lm, se = F, formula = y ~ poly(x, 7)) + 
  labs(title = "7th degree polynomial fit", 
       subtitle = str_c("R^2 = ", summary(lm(incvote ~ poly(juneapp, 7), data = pres))$r.squared %>% round(2))) + 
  coord_cartesian(ylim = range(pres$incvote) + c(-10, 10))
```

---

## Beware overfitting!

With 18 data points, a 7th- or 15th-degree polynomial can (over)fit very well! 

```{r, echo = F, out.width = "80%", out.height="50%"}
pres %>% 
  ggplot(aes(x = juneapp, y = incvote)) +
  geom_point() + 
  geom_smooth(method = lm, se = F, formula = y ~ poly(x, 15)) + 
  labs(title = "15th degree polynomial fit", 
       subtitle = str_c("R^2 = ", summary(lm(incvote ~ poly(juneapp, 15), data = pres))$r.squared %>% round(2))) + 
  coord_cartesian(ylim = range(pres$incvote) + c(-10, 10))
```

---


## Overfitting and out-of-sample prediction

When we overfit, we improve the fit for units in our sample (and maybe others with very similar values of predictors) but worsen our prediction for units not in the sample.

--

The more we overfit, the more our predictions 

--

Remedies: 

- adjusted $R^2$ and other penalties for model complexity
- **cross-validation** to measure out-of-sample fit


---

class: inverse, middle, center

# Appendix (for reference): More about why we minimize the sum of squared residuals

---

## Predicting the value of a random variable

Let $Y$ denote the number of heads in a single flip of a biased coin.

--

Suppose we know the probability mass function of $Y$: 

$$
f(y) = \begin{cases}
1/3 & y = 0 \\\
2/3 & y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
--

The PMF fully describes the random variable.

--

Suppose you wanted to predict the value of $Y$ using a single number: what would you choose?


???

You could choose the mode here, but the expectation is the best choice if we are using MSE as the criterion.

---

## Mean squared error (MSE) as a criterion for prediction

**Definition**: Given random variable $Y$ and scalar $c$, the *mean  squared error* of $Y$ about $c$ is $E[(Y - c)^2]$. (Aronow & Miller, 2.1.22)

--

**Theorem**: The value of $c$ that minimizes MSE of $Y$ about $c$ is $E[Y]$.

--

**Proof**: see proof of theorem 2.1.24 in Aronow & Miller.

???

Basic idea is to decompose MSE of $Y$ wrt $c$ as variance of $Y$ plus squared difference between $E[Y]$ and $c$. To minimize that squared difference, set $c$ equal to $E[Y]$.

--

So our MSE-minimizing prediction for $Y$ is $E[Y] = 2/3$.

$$
f(y) = \begin{cases}
1/3 & y = 0 \\\
2/3 & y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
---

## Populations and samples

To compute $E[Y]$, we need to know the distribution (CDF or PMF/PDF) of $Y$.

--

The distribution of $Y$ is a feature of the  **population** (or *infinite population*).
 
--

In statistics, we typically have a **sample** and want to make inferences about the **population**.

--

Given an iid sample, the sample mean is an unbiased estimator of $E[Y]$ (Theorem 3.2.3 in Aronow & Miller).

--

**Expected value** of a random variable $Y$: 
$$E[Y] = \sum_y y f(y) $$

The **sample mean** of iid random variables $Y_1, Y_2, \ldots, Y_n$:

$$\overline{Y} = \frac{Y_1 + Y_2 + \ldots + Y_n}{n} = \frac{1}{n} \sum_{i = 1}^n Y_i$$

---

## The plug-in principle (simplified)

The **plug-in principle** says that you can (usually) get a good estimate of a statistic defined for the population by substituting "sample quantities" for "population quantities".

--

Population quantity: $E[Y]$ (expectation)
Sample quantity we "plug in": $\overline{Y}$ (sample mean)

--

But we can take the plug-in principle further:

--

Instead of choosing $c$ to minimize MSE (expected squared **error**) 
$$ E[(\underbrace{Y - c}_{\textrm{error}})^2],$$
we can choose $c$ to minimize the sample average of the squared **residuals**:
$$\frac{1}{n} \sum_{i = 1}^n (y_i - c)^2.$$
This is another (convoluted!) way to compute the sample mean.


---

## MSE and SSR

Mean squared error: 

$$ E[(\underbrace{Y - c}_{\textrm{error}})^2]$$

Mean squared residual:

$$ \frac{1}{n} \sum_{i = 1}^n (y_i - c)^2 $$

--

Note the $1/n$ doesn't depend on our prediction, so we often talk about the **sum of squared residuals**.

???

If we started with minimum mean squared error as the objective, why do we end up minimizing the **sum** of squared residuals?

---

class: bg-full
background-image: url("assets/MSE_SSR_diagram_2.png")
background-position: center
background-size: 90%

## Recap via diagram



---

## Illustration

```{r}
set.seed(30500)
y <- rbinom(n = 20, size = 1, prob = 2/3)
y
mean(y) # sample mean
lm(y ~ 1) # simplest OLS regression possible: just the intercept
```

---

class: inverse, middle, center

# One categorical predictor

---

## Predicting the value of a random variable based on a predictor

Let $Y$ denote the number of heads in a single flip of a coin. If $X = 0$, the coin is fair; if $X = 1$, the coin is biased. 

--

The conditional distribution: 

$$
f_{Y|X}(y|x) = \begin{cases}
1/2 & x = 0, y = 0 \\\
1/2 & x = 0, y = 1  \\\
1/4 & x = 1, y = 0 \\\
3/4 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
--

By same logic, 

- knowing the distribution, the best (in terms of MSE) prediction using $X$ is the **conditional expectation** $E[Y \mid X]$
- the sample analog is the **sample mean** at each value of $X$
- minimizing the **sum of squared residuals** (sample analog of MSE) via OLS also gives sample mean at each value of $X$ 

---

## Another illustration

```{r}
n <- 20
dat <- tibble(x = rbinom(n = n, size = 1, 
                         prob = 2/3),
              y = rbinom(n = n, size = 1, 
                  prob = ifelse(x == 0, 1/2, 2/3)))
dat %>% head()

# conditional sample means
dat %>% 
  group_by(x) %>% 
  summarize(mean(y))

lm(y ~ factor(x) - 1, data = dat) # "-1" to drop the intercept
```

---

## Different ways to run the same regression

```{r}
lm(y ~ factor(x) - 1, data = dat)
lm(y ~ factor(x), data = dat)
lm(y ~ x, data = dat)
```


