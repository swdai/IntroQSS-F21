---
title: "PLSC30500, Fall 2021"
subtitle: "5.1 Regression"
# author: "Andy Eggers & Molly Offer-Westort"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: false

---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align = "center", out.width = "900px", fig.width = 4.5, fig.asp = .7)
```


class: inverse, middle, center

# Regression: what it is and how to do it

---

## Regression: what is it for?

In **regression analysis**, we estimate the relationship between a **dependent variable** $(Y_i)$ and **independent variables** $(X_i, D_i, \ldots)$

<br> </br>

Regression **describes** relationships in a dataset, but in some circumstances we can use it for  

- prediction (what will $Y_i$ be, given some $X_i$?)
- causal inference (what is effect of $D_i$ on $Y_i$?)

---

## Ordinary least squares regression

When we say "regression" we often mean **ordinary least squares (OLS)**.

--

Suppose we want to describe $Y_i$ as a function of $D_i$ and $X_i$ in some data: 

$$ \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i $$ 

Note: 

- the " $\hat{}$ "s ("hat"s) indicate estimates; without the " $\hat{}$ " they are population parameters ("truth")
- $\hat{\beta}_0$, $\hat{\beta}_1$, $\hat{\beta}_2$ are "coefficients" or "coefficient estimates"   

--

Define $Y_i - \hat{Y_i} = \hat{r}_i$ as the **residual**. 

--

We seek coefficient estimates that minimize the **sum of squared residuals**, i.e.

$$\underset{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2}{\arg\min} \sum_{i=1}^n  \left(Y_i - (\hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i)\right)^2$$

<!-- Don't know why the math doesn't work. I want \sum_{i=1}^n -->

---

## Toy example 1


.pull-left[

Suppose we have this data: 
```{r get-data, echo = F}
dat <- tribble(~x, ~y,
        0, -1,
        1, 1,
        1, 2)

dat %>% 
  kableExtra::kbl(full_width = F, digits = 2)
```
]

.pull-right[
```{r, out.width = "500px", echo = F}
dat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  expand_limits(x = c(-.25, 1.25), y = c(-1.25, 2.25))

```
]

<br> </br>
<br> </br>

What **intercept** $\hat{\beta}_0$ and **slope** $\hat{\beta}_1$ would minimize the sum of squared residuals? 

What is the minimum sum of squared residuals?

???

At intercept of -1 and slope of 2.5, the SSR is $.5^2 + .5^2 = .5$ 

---

## Toy example 1

```{r}
# base R
lm(y ~ x, data = dat)
# estimatr package
estimatr::lm_robust(y ~ x, data = dat)
```

---

## Toy example 1

```{r, out.width="70%"}
dat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_smooth(method = lm, se = F) + 
  geom_point()
```

---


## Toy example 2

Suppose we have this data: 
```{r get-data-2, out.width = "60%", echo = F}
set.seed(1245)
n <- 100
dat2 <- tibble(x = rnorm(n), 
               y = x + rnorm(n, sd = .8))

dat2 %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(size = 2)  
```

<br> </br>
<br> </br>

What **slope** $\hat{\beta}_0$ and **intercept** $\hat{\beta}_1$ would minimize the sum of squared residuals? 

---

## Toy example 2

```{r}
# base R
lm(y ~ x, data = dat2)
# estimatr package
estimatr::lm_robust(y ~ x, data = dat2)
```

---

## Toy example 2

```{r, out.width="70%"}
dat2 %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_smooth(method = lm, se = F) + 
  geom_point()
```



---

## How does `R` minimize the sum of squared residuals?

Calculus and linear algebra. 

<br> </br>
<br> </br>

(See Linear Models, next course in sequence!)

---

## *Why* does OLS minimize the sum of squared residuals?

Correct but superficial answers: 

- "squared" to make residuals positive
- "squared" because sum of absolute residuals wouldn't always give a unique answer
- "squared" to make the math prettier/easier than e.g. absolute residuals

--

Deeper answers:

- minimizing the sum of squared residuals gives the **conditional mean** or a linear approximation to it; other approaches (e.g. mean absolute residual) do not
- suppose we want a predictor that (in the population) minimizes mean squared error (MSE), i.e. $E[(Y - \hat{Y})^2]$; then the corresponding "plug-in estimator" minimizes the sum of squared residuals (see e.g. Aronow & Miller)
- the predictor that minimizes MSE in the population is the **conditional expectation function (CEF)** $E[Y | X]$; OLS delivers a linear approximation to the CEF 

---

class: bg-full
background-image: url("assets/MSE_SSR_diagram_3.png")
background-position: center
background-size: 75%

## Why sum of squared residuals? 

---

class: inverse, middle, center

# Regression with categorical predictors:  Illustration with an experiment


---

## Butler & Broockman (2011) again

```{r}
bb <- read_csv('../data/legislators_email/Butler_Broockman_AJPS_2011_public_csv.csv')

bb %>% 
  group_by(treat_deshawn) %>% 
  summarize(mean(reply_atall))

lm(reply_atall ~ treat_deshawn, data = bb)

```
How are these related?

---

## From regression output to conditional means 

```{r}
lm(reply_atall ~ treat_deshawn, data = bb)
```

--

The **fitted regression equation** can be written 

$$ \hat{Y_i} = .574 - .018 D_i $$ 
--

So the prediction (conditional mean) depends on the value of $D_i$: 


$$
\hat{Y} = \begin{cases}
.574 & D_i = 0 \\\
.574 - .018 & D_i = 1 
\end{cases}
$$
---

## Another way

Instead of estimating 

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 \mathrm{DeShawn}_i$$
(with emails saying "Jake Mueller" as the omitted group), we can estimate 

$$\hat{Y}_i = \hat{\alpha}_1 \mathrm{NotDeShawn}_i + \hat{\alpha}_2 \mathrm{Deshawn}_i$$
by adding a `-1` to the formula syntax:

```{r}
lm(reply_atall ~ factor(treat_deshawn) - 1, data = bb)
```

---

## Adding a predictor

`leg_republican` indicates whether the email recipient was a Republican.

```{r}
lm(reply_atall ~ treat_deshawn + leg_republican, data = bb)
```

What is the predicted probability of a reply for a "Deshawn" email to a Republican?

--

```{r echo = F}
reg1 <- lm(reply_atall ~ treat_deshawn + leg_republican, data = bb)
coefs <- coef(reg1) %>% round(3)
```

The prediction is $`r coefs[1]` + `r coefs[2]` + `r coefs[3]` = `r sum(coefs)`$.

---

## Adding a predictor (2)

```{r}
lm(reply_atall ~ treat_deshawn + leg_republican, data = bb) %>% 
  coef()
```

If we write the prediction equation as  $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 \mathrm{DeShawn}_i + \hat{\beta}_2 \mathrm{LegRep}_i$, then we have

| Email | Recipient | Predicted response rate | 
|-------|:---------:|:-----------------------:|
| Jake  |   Dem     |      $\hat{\beta}_0$    |
| DeShawn  |   Dem     |      $\hat{\beta}_0 + \hat{\beta}_1$   |
| Jake  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_2$    |
| DeShawn  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_2$    |

--

So what is the assumed DeShawn-vs-Jake difference for Democrats? For Republicans? 

---

## We add an interaction 

<!-- To avoid imposing the assumption that Democrats and Republicans respond the same way to DeShawn-vs-Jake, we add an **interaction**. -->

```{r}
lm(reply_atall ~ treat_deshawn*leg_republican, data = bb) %>% 
  coef()
```

The prediction equation becomes 

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 \mathrm{DeShawn}_i + \hat{\beta}_2 \mathrm{LegRep}_i + \hat{\beta}_3 \mathrm{DeShawn}_i  \times \mathrm{LegRep}_i$$
so we have

| Email | Recipient | Predicted response rate | 
|-------|:---------:|:-----------------------:|
| Jake  |   Dem     |      $\hat{\beta}_0$    |
| DeShawn  |   Dem     |      $\hat{\beta}_0 + \hat{\beta}_1$   |
| Jake  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_2$    |
| DeShawn  |   Rep     |      $\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_2 + \hat{\beta}_3$    |

---

## Interpreting interactions 

Compare: 

```{r}
lm(reply_atall ~ treat_deshawn*leg_republican, data = bb) %>% 
  coef()
```

```{r}
bb %>% 
  group_by(treat_deshawn, leg_republican) %>% 
  summarize(resp_rate = mean(reply_atall))
```


---

## Summing up

- in a regression using only categorical explanatory variables, the predictions are just sample means for categories
- to recover the predictions/means, you usually have to add up coefficients: this can be tricky

---

## $R^2$ and overfitting 

Here is a dataset on U.S. presidents: 

```{r}
pres <- read_csv("./../data/pres_data.csv")

```

| Variable Name | Description | 
|-------:|:-----------------------:|
|year | Election year |
| deminc | 1=incumbent is a Democrat |
| incvote | Percent of the two-party vote for the incumbent party |
|q2gdp | Second-quarter change in real GDP in election year |
| juneapp | Net approval of incumbent president in June of election year |

---

## Incumbent approval rating and incumbent party vote share 

```{r, out.width="70%"}
vote_app %>% 
  ggplot(aes(x = juneapp, y = incvote, label = year)) + 
  geom_point() + 
  geom_text(nudge_x = 2, nudge_y = .5, size = 2)
vote_app + 
  geom_smooth(method = lm, se = F)
```

---

# Standard `R` regression output

```{r}
lm(incvote ~ juneapp, data = pres) %>% 
  summary()
```


---

# Standard `R` regression output

```{r}
estimatr::lm_robust(incvote ~ juneapp, data = pres) %>% 
  summary()
```

---

## $R^2$

$R^2$ measures the proportion of variance in the dependent variable "explained" by the independent variables. 



# R^2 and overfitting.
```



```{r}
cces <- readRDS("./../data/cces/cumulative_2006-2020.rds")
cces %>%
  filter(year %in% c(2014) & vv_turnout_gvm %in% c("Voted", "No Record of Voting") & age < 90) %>%
  group_by(age, year) %>% 
  summarize(weighted = weighted.mean(vv_turnout_gvm == "Voted", w = weight),
            unweighted = mean(vv_turnout_gvm == "Voted")) %>%
  pivot_longer(cols = c(weighted, unweighted)) %>% 
  ggplot(aes(x = age, y = value, col = name)) + 
  geom_point(size = 1) + 
  geom_line(alpha = .5)
  

```


For problem set?

```{r}
lm(reply_atall ~ treat_deshawn, data = bb, subset = treat_noprimary == 1) 

bb %>% 
  mutate(primary = case_when(treat_demprimary == 1 ~ "dem",
                             treat_repprimary == 1 ~ "rep",
                             treat_noprimary == 1 ~ "none"))  -> bb2

lm(reply_atall ~ treat_deshawn*primary, data = bb2) %>% 
  broom::tidy()

```


---

class: inverse, middle, center

# Appendix (for reference): More about why we minimize the sum of squared residuals

---

## Predicting the value of a random variable

Let $Y$ denote the number of heads in a single flip of a biased coin.

--

Suppose we know the probability mass function of $Y$: 

$$
f(y) = \begin{cases}
1/3 & y = 0 \\\
2/3 & y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
--

The PMF fully describes the random variable.

--

Suppose you wanted to predict the value of $Y$ using a single number: what would you choose?


???

You could choose the mode here, but the expectation is the best choice if we are using MSE as the criterion.

---

## Mean squared error (MSE) as a criterion for prediction

**Definition**: Given random variable $Y$ and scalar $c$, the *mean  squared error* of $Y$ about $c$ is $E[(Y - c)^2]$. (Aronow & Miller, 2.1.22)

--

**Theorem**: The value of $c$ that minimizes MSE of $Y$ about $c$ is $E[Y]$.

--

**Proof**: see proof of theorem 2.1.24 in Aronow & Miller.

???

Basic idea is to decompose MSE of $Y$ wrt $c$ as variance of $Y$ plus squared difference between $E[Y]$ and $c$. To minimize that squared difference, set $c$ equal to $E[Y]$.

--

So our MSE-minimizing prediction for $Y$ is $E[Y] = 2/3$.

$$
f(y) = \begin{cases}
1/3 & y = 0 \\\
2/3 & y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
---

## Populations and samples

To compute $E[Y]$, we need to know the distribution (CDF or PMF/PDF) of $Y$.

--

The distribution of $Y$ is a feature of the  **population** (or *infinite population*).
 
--

In statistics, we typically have a **sample** and want to make inferences about the **population**.

--

Given an iid sample, the sample mean is an unbiased estimator of $E[Y]$ (Theorem 3.2.3 in Aronow & Miller).

--

**Expected value** of a random variable $Y$: 
$$E[Y] = \sum_y y f(y) $$

The **sample mean** of iid random variables $Y_1, Y_2, \ldots, Y_n$:

$$\overline{Y} = \frac{Y_1 + Y_2 + \ldots + Y_n}{n} = \frac{1}{n} \sum_{i = 1}^n Y_i$$

---

## The plug-in principle (simplified)

The **plug-in principle** says that you can (usually) get a good estimate of a statistic defined for the population by substituting "sample quantities" for "population quantities".

--

Population quantity: $E[Y]$ (expectation)
Sample quantity we "plug in": $\overline{Y}$ (sample mean)

--

But we can take the plug-in principle further:

--

Instead of choosing $c$ to minimize MSE (expected squared **error**) 
$$ E[(\underbrace{Y - c}_{\textrm{error}})^2],$$
we can choose $c$ to minimize the sample average of the squared **residuals**:
$$\frac{1}{n} \sum_{i = 1}^n (y_i - c)^2.$$
This is another (convoluted!) way to compute the sample mean.


---

## MSE and SSR

Mean squared error: 

$$ E[(\underbrace{Y - c}_{\textrm{error}})^2]$$

Mean squared residual:

$$ \frac{1}{n} \sum_{i = 1}^n (y_i - c)^2 $$

--

Note the $1/n$ doesn't depend on our prediction, so we often talk about the **sum of squared residuals**.

???

If we started with minimum mean squared error as the objective, why do we end up minimizing the **sum** of squared residuals?

---

class: bg-full
background-image: url("assets/MSE_SSR_diagram_2.png")
background-position: center
background-size: 90%

## Recap via diagram



---

## Illustration

```{r}
set.seed(30500)
y <- rbinom(n = 20, size = 1, prob = 2/3)
y
mean(y) # sample mean
lm(y ~ 1) # simplest OLS regression possible: just the intercept
```

---

class: inverse, middle, center

# One categorical predictor

---

## Predicting the value of a random variable based on a predictor

Let $Y$ denote the number of heads in a single flip of a coin. If $X = 0$, the coin is fair; if $X = 1$, the coin is biased. 

--

The conditional distribution: 

$$
f_{Y|X}(y|x) = \begin{cases}
1/2 & x = 0, y = 0 \\\
1/2 & x = 0, y = 1  \\\
1/4 & x = 1, y = 0 \\\
3/4 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
--

By same logic, 

- knowing the distribution, the best (in terms of MSE) prediction using $X$ is the **conditional expectation** $E[Y \mid X]$
- the sample analog is the **sample mean** at each value of $X$
- minimizing the **sum of squared residuals** (sample analog of MSE) via OLS also gives sample mean at each value of $X$ 

---

## Another illustration

```{r}
n <- 20
dat <- tibble(x = rbinom(n = n, size = 1, 
                         prob = 2/3),
              y = rbinom(n = n, size = 1, 
                  prob = ifelse(x == 0, 1/2, 2/3)))
dat %>% head()

# conditional sample means
dat %>% 
  group_by(x) %>% 
  summarize(mean(y))

lm(y ~ factor(x) - 1, data = dat) # "-1" to drop the intercept
```

---

## Different ways to run the same regression

```{r}
lm(y ~ factor(x) - 1, data = dat)
lm(y ~ factor(x), data = dat)
lm(y ~ x, data = dat)
```


