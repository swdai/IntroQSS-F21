---
title: "PLSC30500, Fall 2021"
subtitle: "5.1 Regression"
# author: "Andy Eggers & Molly Offer-Westort"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: false

---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align = "center", out.width = "900px", fig.width = 4.5, fig.asp = .7)
```


class: inverse, middle, center

# Regression: what it is and how to do it

---

## Regression: what is it for?

In **regression analysis**, we estimate the relationship between a **dependent variable** $(Y_i)$ and **independent variables** $(X_i, D_i, \ldots)$

<br> </br>

Regression **describes** the relationship between $Y_i$ and $\mathbf{X}_i$ in a dataset, but in some circumstances we can use it for  

- prediction (what will $Y_i$ be, given some $X_i$?)
- causal inference (what is effect of $D_i$ on $Y_i$?)

---

## Ordinary least squares regression

When we say "regression" we often mean **ordinary least squares (OLS)**.

--

Suppose we want to describe $Y_i$ as a function of $D_i$ and $X_i$ given $n$ observations: 

$$ \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i $$ 

Note: 

- the " $\hat{}$ "s ("hat"s) indicate estimates; without the " $\hat{}$ " they are population parameters ("truth")
- $\hat{\beta}_0$, $\hat{\beta}_1$, $\hat{\beta}_2$ are "coefficients" or "coefficient estimates"   

--

Define $Y_i - \hat{Y_i} = \hat{r}_i$ as the **residual**. 

--

We seek coefficient estimates that minimize the **sum of squared residuals**, i.e.

$$ \underset{\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2}{\arg\min} \sum_i  \left(Y_i - (\hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i)\right)^2 $$

<!-- Don't know why the math doesn't work. I want \sum_{i=1}^n -->

---

## Toy example 1


.pull-left[

Suppose we have this data: 
```{r get-data, echo = F}
dat <- tribble(~x, ~y,
        0, -1,
        1, 1,
        1, 2)

dat %>% 
  kableExtra::kbl(full_width = F, digits = 2)
```
]

.pull-right[
```{r, out.width = "500px", echo = F}
dat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  expand_limits(x = c(-.25, 1.25), y = c(-1.25, 2.25))

```
]

<br> </br>
<br> </br>

What **intercept** $\hat{\beta}_0$ and **slope** $\hat{\beta}_1$ would minimize the sum of squared residuals? 

---

## Toy example 1 (2)

```{r}
# base R
lm(y ~ x, data = dat)
# estimatr package
estimatr::lm_robust(y ~ x, data = dat)
```

---

## Toy example 1 (3)

```{r, out.width="70%"}
dat %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_smooth(method = lm, se = F) + 
  geom_point()
```

---


## Toy example 2

Suppose we have this data: 
```{r get-data-2, out.width = "60%"}
set.seed(1245)
n <- 100
dat2 <- tibble(x = rnorm(n), 
               y = x + rnorm(n, sd = .8))

dat2 %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(size = 2)  
```

<br> </br>
<br> </br>

What **slope** $\hat{\beta}_0$ and **intercept** $\hat{\beta}_1$ would minimize the sum of squared residuals? 

---

## Toy example 2 (2)

```{r}
# base R
lm(y ~ x, data = dat2)
# estimatr package
estimatr::lm_robust(y ~ x, data = dat2)
```

---

## Toy example 2 (3)

```{r, out.width="70%"}
dat2 %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_smooth(method = lm, se = F) + 
  geom_point()
```

---

## How does `R` minimize the sum of squared residuals?

Calculus and linear algebra. 

<br> </br>
<br> </br>

(See Linear Models, next course in sequence!)

---



## Interview question

Why is OLS "ordinary least **squares**?"

---

class: inverse, middle, center

# No predictors

---


## Predicting the value of a random variable

Let $Y$ denote the number of heads in a single flip of a biased coin.

--

Suppose we know the probability mass function of $Y$: 

$$
f(y) = \begin{cases}
1/3 & y = 0 \\\
2/3 & y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
--

The PMF fully describes the random variable.

--

Suppose you wanted to predict the value of $Y$ using a single number: what would you choose?


???

You could choose the mode here, but the expectation is the best choice if we are using MSE as the criterion.

---

## Mean squared error (MSE) as a criterion for prediction

**Definition**: Given random variable $Y$ and scalar $c$, the *mean  squared error* of $Y$ about $c$ is $E[(Y - c)^2]$. (Aronow & Miller, 2.1.22)

--

**Theorem**: The value of $c$ that minimizes MSE of $Y$ about $c$ is $E[Y]$.

--

**Proof**: see proof of theorem 2.1.24 in Aronow & Miller.

???

Basic idea is to decompose MSE of $Y$ wrt $c$ as variance of $Y$ plus squared difference between $E[Y]$ and $c$. To minimize that squared difference, set $c$ equal to $E[Y]$.

--

So our MSE-minimizing prediction for $Y$ is $E[Y] = 2/3$.

$$
f(y) = \begin{cases}
1/3 & y = 0 \\\
2/3 & y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
---

## Populations and samples

To compute $E[Y]$, we need to know the distribution (CDF or PMF/PDF) of $Y$.

--

The distribution of $Y$ is a feature of the  **population** (or *infinite population*).
 
--

In statistics, we typically have a **sample** and want to make inferences about the **population**.

--

Given an iid sample, the sample mean is an unbiased estimator of $E[Y]$ (Theorem 3.2.3 in Aronow & Miller).

--

**Expected value** of a random variable $Y$: 
$$E[Y] = \sum_y y f(y) $$

The **sample mean** of iid random variables $Y_1, Y_2, \ldots, Y_n$:

$$\overline{Y} = \frac{Y_1 + Y_2 + \ldots + Y_n}{n} = \frac{1}{n} \sum_{i = 1}^n Y_i$$

---

## The plug-in principle (simplified)

The **plug-in principle** says that you can (usually) get a good estimate of a statistic defined for the population by substituting "sample quantities" for "population quantities".

--

Population quantity: $E[Y]$ (expectation)
Sample quantity we "plug in": $\overline{Y}$ (sample mean)

--

But we can take the plug-in principle further:

--

Instead of choosing $c$ to minimize MSE (expected squared **error**) 
$$ E[(\underbrace{Y - c}_{\textrm{error}})^2],$$
we can choose $c$ to minimize the sample average of the squared **residuals**:
$$\frac{1}{n} \sum_{i = 1}^n (y_i - c)^2.$$
This is another (convoluted!) way to compute the sample mean.


---

## MSE and SSR

Mean squared error: 

$$ E[(\underbrace{Y - c}_{\textrm{error}})^2]$$

Mean squared residual:

$$ \frac{1}{n} \sum_{i = 1}^n (y_i - c)^2 $$

--

Note the $1/n$ doesn't depend on our prediction, so we often talk about the **sum of squared residuals**.

???

If we started with minimum mean squared error as the objective, why do we end up minimizing the **sum** of squared residuals?

---

class: bg-full
background-image: url("assets/MSE_SSR_diagram_2.png")
background-position: center
background-size: 90%

## Recap via diagram



---

## Illustration

```{r}
set.seed(30500)
y <- rbinom(n = 20, size = 1, prob = 2/3)
y
mean(y) # sample mean
lm(y ~ 1) # simplest OLS regression possible: just the intercept
```

---

class: inverse, middle, center

# One categorical predictor

---

## Predicting the value of a random variable based on a predictor

Let $Y$ denote the number of heads in a single flip of a coin. If $X = 0$, the coin is fair; if $X = 1$, the coin is biased. 

--

The conditional distribution: 

$$
f_{Y|X}(y|x) = \begin{cases}
1/2 & x = 0, y = 0 \\\
1/2 & x = 0, y = 1  \\\
1/4 & x = 1, y = 0 \\\
3/4 & x = 1, y = 1 \\\
0 & \text{otherwise}
\end{cases}
$$
--

By same logic, 

- knowing the distribution, the best (in terms of MSE) prediction using $X$ is the **conditional expectation** $E[Y \mid X]$
- the sample analog is the **sample mean** at each value of $X$
- minimizing the **sum of squared residuals** (sample analog of MSE) via OLS also gives sample mean at each value of $X$ 

---

## Another illustration

```{r}
n <- 20
dat <- tibble(x = rbinom(n = n, size = 1, 
                         prob = 2/3),
              y = rbinom(n = n, size = 1, 
                  prob = ifelse(x == 0, 1/2, 2/3)))
dat %>% head()

# conditional sample means
dat %>% 
  group_by(x) %>% 
  summarize(mean(y))

lm(y ~ factor(x) - 1, data = dat) # "-1" to drop the intercept
```

---

## Different ways to run the same regression

```{r}
lm(y ~ factor(x) - 1, data = dat)
lm(y ~ factor(x), data = dat)
lm(y ~ x, data = dat)
```

---

class: inverse, middle, center

# Two categorical predictors

---


