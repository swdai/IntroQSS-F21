---
title: "PLSC30500, Fall 2021"
subtitle: "6.1 Regression for causal inference"
# author: "Andy Eggers & Molly Offer-Westort"
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE, fig.align = "center", out.width = "900px", fig.width = 4.5, fig.asp = .7)
```

```{css, echo = F}

.small-output .remark-code{
  font-size: small;
}
```



## Conditional average treatment effect (review)

Recall Broockman & Butler experiment from lecture 4.2.

We considered *conditional average treatment effects*, e.g. 

$$E[Y_i(1) - Y_i(0) | \mathrm{party} = \mathrm{Dem}]$$
--

Randomization means treatment $D_i$ is independent of potential outcomes $Y_i(0), Y_i(1)$. 

--

So the *conditional difference in means* is an unbiased estimator of this and other CATEs: 

\begin{align*}
E[Y_i(1) - Y_i(0) | X_i = x] &= \underbrace{E[Y_i(1) | X_i = x ]}_{\text{Avg. }Y_i(1)\text{ for units with }X_i = x} - \underbrace{E[Y_i(0) | X_i = x]}_{\text{Avg. }Y_i(0)\text{ for units with }X_i = x} \\
&= E[Y_i(1) | D_i = 1, X_i = x ] - E[Y_i(0) | D_i = 0, X_i = x] \\
&= \underbrace{E[Y_i | D_i = 1, X_i = x ]}_{\text{Avg. }Y_i\text{ for units with }D_1 = 1, X_i = x} - \underbrace{E[Y_i | D_i = 0, X_i = x]}_{\text{Avg. }Y_i\text{ for units with }D_1 = 0, X_i = x} \\
\end{align*}

???

To go from 1 to 2: $D_i$ independent of $Y_i(0),Y_i(1)$
To go from 2 to 3: potential outcomes model, i.e. $Y_i = D_i Y_i(1) + (1 - D_i)Y_i(0)$

---

## Conditional ignorability

In some cases, independence of potential outcomes and treatment

$$\left(Y_i(1), Y_i(0)\right) \perp\!\!\!\!\perp  D_i$$ 
may not hold *in general*, but it may hold *conditional on some* $X_i$: 

$$\left(Y_i(1), Y_i(0)\right) \perp\!\!\!\!\perp  D_i \mid X_i.$$ 
Then $D_i$ is *strongly ignorable* conditional on $X_i$.

--

**Example**: Suppose in Broockman & Butler the randomization went like this:

\begin{align*}
\text{Pr}[\text{DeShawn email} | \text{Dem legislator}] &= .4 \\
\text{Pr}[\text{DeShawn email} | \text{Rep legislator}] &= .6 \\
\end{align*}

--

Then treatment is related to party (which is probably related to potential outcomes), but unrelated to potential outcomes *conditional on* party. 

???

It is called *ignorability* because you can ignore the way that treatment was assigned.

$\left(Y_i(1), Y_i(0)\right) \perp\!\!\!\!\perp  D_i \mid X_i$ is the conditional independence assumption. Technically you also need positivity: $0 < \text{Pr}[D_i = 1 | X_i] < 1$. 

**Strong** ignorability relates to the pair $Y_i(0), Y_i(1)$. **Weak** ignorability in the binary treatment case says $Y_i(1) \perp\!\!\!\!\perp  D_i \mid X_i$ and $Y_i(0) \perp\!\!\!\!\perp  D_i \mid X_i$, so it permits a relationship between the treatment effect and $D_i$ conditional on $X_i$. But note that the key results would work with weak ignorability.  

---

## Another example of conditional ignorability 

Recall our example of the effect of studying on final grades:

- AB types: $Y(1) = 95$, $Y(0) = 85$
- BC types: $Y(1) = 85$, $Y(0) = 75$

--

Suppose most AB types study and most BC types don't. 

--

Then overall difference-in-means will be biased (which way?).

--

But treatment is strongly ignorable conditional on type (because potential outcomes identical within types).

---

## Conditional ignorability and the CATE

If treatment is strongly ignorable conditional on $X_i$, then treatment is *as-if random* within levels of $X_i$.

--

$\implies$ the conditional difference in means is an unbiased estimator of the conditional average treatment effect, $\mathrm{CATE}$.

--

So 

| Study | Difference in means unbiased for ATE? | Conditional difference in means unbiased for CATE? | 
|:---:|:---:| :----:|
|Original Broockman and Butler | Yes | Yes |
|Modified Broockman and Butler | No | Yes |

---

## From conditional ignorability to the ATE

Wait: if you have an unbiased estimator for the conditional ATE at each value of $X_i$, don't you have an unbiased estimator for the ATE? 

--

For example: 

- to estimate the ATE of the "DeShawn" email on response rates, take the difference in means separately for Dem and Rep legislators, and then combine them
- to estimate the ATE of studying on academic performance, take the difference in means separately for AB types and BC types, and then combine them
- to estimate ATE of peacekeeping on civil conflict, take the difference in means for types of countries (e.g. low previous conflict, medium previous conflict) and then combine them

--

This is what we mean by "controlling for $X$" or "conditioning on $X$" to get the ATE.

---

## From conditional ignorability to the ATE (more formally)

By the *Law of Iterated Expectations (LIE)*,

\begin{align*}
\text{ATE} &\equiv E[\tau_i] \\
&= \sum_x E[\tau_i | X_i = x] \text{Pr}[X_i = x]
\end{align*}

--


Other examples of LIE:

- to compute average age in our class, can compute average age for men and average age for women, then combine by share of men/women 
- to compute world average GDP/capita, compute for each country separately, then combine by population shares 

--

So if strong ignorability holds, an unbiased estimator for ATE is the weighted average of differences in means.

---

## The math (if it helps)

\begin{align*}
\text{ATE} &\equiv E[\tau_i] \\ 
&= \sum_x E[\tau_i | X_i = x] \mathrm{Pr}[X_i = x]  \\
&= \sum_x E[Y_i(1) -  Y_i(0)| X_i = x] \mathrm{Pr}[X_i = x] \\
&= \sum_x \left( E[Y_i(1) | X_i = x] - E[Y_i(0)| X_i = x] \right) \mathrm{Pr}[X_i = x]  \\
&= \sum_x \left( E[Y_i(1) | D_i = 1, X_i = x] - E[Y_i(0)| D_i = 0, X_i = x] \right) \mathrm{Pr}[X_i = x] \\
&= \sum_x \left( E[Y_i | D_i = 1, X_i = x] - E[Y_i| D_i = 0, X_i = x] \right) \mathrm{Pr}[X_i = x]  
\end{align*}

Note:

- 1 is by definition of ATE
- 2 is by law of iterated expectations
- 3 is by definition of ITE
- 4 is by linearity of expectations
- 5 is by assumption of potential outcomes model (SUTVA)
- 6 is by strong ignorability

See Theorem 7.1.13 in Aronow and Miller.

---

## Back to regression

Regression provides a convenient way to compute and combine differences in means conditional on covariates $X_1$, $X_2$, etc.

--

For example, the regression equation

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\tau} D_i + \hat{\beta}_1 X_{i1} + \hat{\beta}_2 X_{i2}$$ 

lets us calculate conditional differences in (predicted) means 

$$\hat{E}[Y_i | D_i = 1, X_{i1} = x_1, X_{i2} = x_2] - \hat{E}[Y_i | D_i = 0, X_{i1} = x_1, X_{i2} = x_2] = \hat{\tau}.$$ 

--

If conditional ignorability holds, this is an unbiased estimate of a CATE.

--

The regression equation above assumes CATEs are all the same; if so, this regression gives unbiased estimate of ATE; if not, it gives a weighted average of CATEs.

???

If conditional ignorability holds and you have categorical data with a saturdated model but no interactions with treatment, then I think the coefficient on treatment is a weighted average of the CATEs, where weights are according to variance of treatment in each cell. See Mostly Harmless and Matt Blackwell's notes.

---

## Regression and other predictive methods

We have described regression (OLS) as a way to predict $Y$ as a function of $D_i$, $X_i$, etc.

--

Given conditional ignorability, this can yield unbiased estimates of the ATE. 

--

There are other ways to predict $Y$ as a function $D_i$, $X_i$, etc, including: 

- maximum likelihood estimation (MLE)
- matching
- weighting
- machine learning methods (e.g. random forests, neural nets)

Take more courses to find out more!

---

## Conditional ignorability and DAGs

Recall: "treatment is ignorable conditional on $\mathbf{X}_i$" means 

$(Y_i(0), Y_i(1))  \perp\!\!\!\!\perp D_i | \mathbf{X}_i$

--

The DAG version of this that that the only unblocked path from $D$ to $Y$ is the treatment effect of interest.

```{r, fig.height=3, fig.width = 4, fig.align = "center", echo = F, message = F, out.width = "80%"}
library(ggdag)
coords <- tibble::tribble(
  ~name, ~x,  ~y,
  "D", 0,   0,
  "X", -.25,   .25,
  "Y",  1,   0,
  "W", .5, -.25
)

dagify(Y ~ X + D,
       D ~ X,
       W ~ D,
       W ~ Y,
       coords = coords)%>% 
  tidy_dagitty() %>% 
  ggdag() + 
  theme_void()

```


---

## Another DAG

```{r, fig.height=3, fig.width = 4, fig.align = "center", echo = F, message = F, out.width = "80%"}
coords <- tibble::tribble(
  ~name, ~x,  ~y,
  "D", 0,   0,
  "X1", -.15,   .25,
  "X2", -.35,   .35,
  "X3", -.55,   .45,
  "Y",  1,   0,
  "W", .5, -.25
)

dagify(Y ~ X1 + X2 + X3 + D,
       D ~ X1 + X2 + X3,
       coords = coords)%>% 
  tidy_dagitty() %>% 
  ggdag() + 
  theme_void()

```


---

## The hard part: satisfying conditional ignorability

In applied settings, can you observe the covariates that make conditional ignorability plausible? 

i.e. such that treatment is as-if random conditional on $\mathbf{X}_i$

i.e. such that the effect of $D_i$ on $Y_i$ is the only unblocked path connecting them?

--

For example

- if studying effect of democracy on economic growth, is it enough to control for past growth?
- if studying effect of attending private university on future earnings, is it enough to control for the exact set of universities to which the student was admitted? (example from Angrist & Pischke, *Mastering 'Metrics*)

--

Take causal inference.

???

This is why people end up doing experiments and looking for natural experiments, RDD, IV, diff-in-diff, etc.

---

## What should you control for?

From potential outcomes/ignorability standpoint or DAG standpoint, should be clear that you should control for variables that 

- affect the treatment, *and*
- affect the outcome

--

Don't control for other effects of the treatment ("post-treament variables").

---

## What should you control for? An OVB perspective

In seminars, we're often focused on a regression coefficient.

A common critique is "But you didn't control for $X$". 

--

A useful way to assess how (not) controlling for $X$ might affect a coefficient of interest: the omitted variable bias (OVB) formula. 

---

## The omitted bias formula

Consider three fitted regression equations: 

\begin{align*}
Y_i &= \hat{\beta}_0 + \hat{\beta}_1 D_i + \hat{\beta}_2 X_i + \hat{r}_i \qquad \text{(long eqn)} \\
Y_i &= \hat{\gamma}_0 + \hat{\gamma}_1 D_i + \hat{\varepsilon}_i \qquad \quad \text{(short eqn)} \\
X_i &= \hat{\alpha}_0 + \hat{\alpha}_1 D_i + \hat{u}_i \qquad \text{(auxiliary eqn)}
\end{align*}

OVB equation says: $\hat{\gamma}_1 = \hat{\beta}_1 + \hat{\beta}_2 \hat{\alpha}_1$.

**Shorthand:** "short" coefficient is "long" coefficient plus "impact" (of $X$ on $Y$) times  "imbalance" (in $X$ across $D$). 

--

**Proof**: Substitute auxiliary equation into long equation and simplify:

\begin{align*}
\hat{Y}_i &= \hat{\beta}_0 + \hat{\beta}_1 D_i  + \hat{\beta}_2 (\hat{\alpha}_0 + \hat{\alpha}_1 D_i + \hat{u}_i) + \hat{r}_i \\
&= \hat{\beta}_0 + \hat{\beta}_2 \hat{\alpha}_0 + (\hat{\beta}_1 +  \hat{\beta}_2 \hat{\alpha}_1 ) D_i + \hat{\beta}_2 \hat{u}_i + \hat{r}_i
\end{align*}

---

## Illustration of OVB formula

```{r}
pres <- read_csv("./../data/pres_data.csv") %>% filter(year < 2020)
long <- lm(incvote ~ juneapp + q2gdp, data = pres)
coef(long)
```
--
```{r}
short <- lm(incvote ~ juneapp, data = pres)
coef(short)
```
--
```{r}
auxiliary <- lm(q2gdp ~ juneapp, data = pres)
coef(auxiliary)
```
--
```{r}
coef(long)["juneapp"] + coef(long)["q2gdp"]*coef(auxiliary)["juneapp"] 
```



