---
title: 7.2 Inference for Random Samples
subtitle: PLSC30500, Fall 2021
author: 
  # - co-taught by Molly Offer-Westort & Andy Eggers
  # - .small-text[(This lecture with references to Aronow & Miller 2019)]
output: 
  xaringan::moon_reader:
    self_contained: true
    css: [default, uchicago_pol_meth.css]
    nature:
      highlightLines: true
      countIncrementalSlides: no
---

```{r setup, include=FALSE}
library(tidyverse)
set.seed(60637)
options(width = 60)
```

```{css, echo=FALSE}
.small-output .remark-code{
  font-size: x-small;
}

# .show-only-last-code-result pre + pre:not(:last-of-type) code[class="remark-code"] {
#     display: none;
# }
```

# 

Last class, we assumed we had a finite population that we observe all of, and the source of randomness in what we observed was due to random assignment of treatment. This is called *randomization inference.*

--

Now, we'll assume that our data is produced from a random generative process, where we're sampling from some (potentially infinite) population distribution that is not fully observed. This is the type of inference we use for survey sampling. 

--

It's important to consider *what the source of randomness is* and *what population we're making inferences about*. 

---


# Back to estimation


Returning to our example where we flip a coin twice, let $X$ be the number of heads we observe. Our coin is *not* fair, and the probability of getting a heads is 0.8. 

--

The random variable's probability distribution is then:

$$
f(x) = \begin{cases}
1/16 & x = 0 \\\
3/8 & x = 1 \\\
9/16 & x = 2 \\\
0 & \text{otherwise}
\end{cases}
$$

---

Let's take a look at the mean. 

```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}
plotdata <- tibble(
  x = c(0, 1, 2),
  xend = c(1, 2, 3),
  fx = c(1/16, 3/8, 9/16),
  Fx = cumsum(fx)
)

# Expected value
Ex <- sum(plotdata$x*plotdata$fx)

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +  
  geom_vline(xintercept = 1.5, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=1.5, y=0.75, label="E[X]") +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
```

--

$$
\begin{align}
\textrm{E}[X] & = \sum_x x fx \\\\
& = 0 \times \frac{1}{16} + 1 \times \frac{3}{8} + 2 \times \frac{9}{16}\\\
& = \frac{24}{16}\\\
& = 1.5
\end{align}
$$

---

And the spread.  

```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +  
  geom_vline(xintercept = Ex, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=Ex, y=0.75, label="E[X]", color = 'grey') +
  geom_segment(aes(x = Ex, xend = Ex+sign(x-Ex)*(x-Ex)^2, y = fx, yend = fx), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  annotate(geom="text", x=1.8, y=0.63, label="Squared distance\nfrom mean") +
  annotate(geom="text", x=(plotdata$x+Ex)/2, y=(plotdata$fx-0.05), label=(plotdata$x-Ex)^2, color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
```

Variance = average squared distance from the mean
$$
\begin{align}
\textrm{Var}[X] & = \textrm{E}[(X - \textrm{E}[X)^2]\\\\
& = 2.25 \times \frac{1}{16} + 0.25 \times \frac{3}{8} + 0.25 \times \frac{9}{16} \\\\
& = 0.375
\end{align}
$$


---
And the spread.  

```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}

sdx <- sqrt(sum((plotdata$x-Ex)^2 *plotdata$fx))

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.8, 2.8),
                  ylim = c(0,1)) +
  geom_segment(aes(x = x, y = c(0,0,0), xend = x, yend = fx)) +  
  geom_vline(xintercept = Ex, lty = 'dashed', color = 'skyblue') +
  annotate(geom="text", x=Ex, y=0.75, label="E[X]", color = 'grey') +
  geom_segment(aes(x = Ex, xend = Ex-sdx, y = 0.5, yend = 0.5), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  geom_segment(aes(x = Ex, xend = Ex+sdx, y = 0.5, yend = 0.5), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = Ex, y = fx), color = 'skyblue') +
  annotate(geom="text", x=1.8, y=0.63, label="Square root of average\nsquared distance\nfrom mean") +
  annotate(geom="text", x=(Ex+c(-1.05,1.05)*round(sdx, 3)/2), y=0.45, 
           label=round(sdx, 3), color = 'skyblue') +
  ggtitle('PMF of X as number of heads in 2 UNfair coin flips')
```

SD = square root of variance
$$
= \sqrt{0.375} = 0.612
$$

---


We can check our calculations of the expectation and spread in R. 

--

First, we'll want to simulate the random process. 

```{r}
n <- 1000
X <- c(0, 1, 2)
probs <- c(1/16, 3/8, 9/16)
x_observed <- sample(X, prob = probs, replace = TRUE, size = n)

head(x_observed)
```

--


```{r}
mean(x_observed)
```

--

```{r}
var(x_observed)
sd(x_observed)
```

---

The process that we just did -- sampling and estimation based on observed data -- is a very common process in empirical research. 

--

But we may notice that the mean, variance, and standard deviation are not exactly what we calculated analytically. 

---

Let's try it again. 

--


```{r}
x_observed <- sample(X, prob = probs, replace = TRUE, size = n)

mean(x_observed)
var(x_observed)
sd(x_observed)
```

---

The values that we get are close, but not identical. 

--

This is because what we are observing in practice is a *sample* from the data. 


---

# Sampling and statistics

Very often, we only observe a limited number of observations, which are drawn from a larger population. 

--
Review:

We can summarize the data we observe with *statistics*. Statistics are functions of the data we observe.
--


$$
T_n = h(X_1, \dots, X_n)
$$

--

(Estimators are a class of statistics that we use to approximate specific estimands. Test statistics are the specific statistics we use to test hypotheses. )

--

Because our sampling process is a random process, these statistics themselves are random variables, with their own distributions. 

--

We can describe our sample, but we might also like to *make inferences* about the larger population--i.e., to summarize what we know about that population based on the data we observe. 

--
This is what we use statistics for, and why we talk about probability AND statistics. 

--

Probability gives us a model of the world. 

--
Statistics give us a way to relate the data that we see to the model. 

---

In our two coin flip example, suppose we don't know whether the coin is fair or not. We can observe the results of a large number of coin flips, and make an educated guess about the underlying population value. 

--

Formally, that educated guess is called *estimation*. 

---

## i.i.d. Data

The standard treatment of estimation in statistics is built around the assumption that observations are *independent and identically distributed*. 

--

Formally, if we have $n$ draws, $X_1, \dots, X_n$, these draws are i.i.d. if they are independent from each other, and all have the same CDF. 

$$
X_1, \dots, X_n \sim F_X
$$

*Notational aside: $~$ is read as "distributed," and means that the random variable $X$ has the distribution function $F$.*

---

In our coin flip example, each time we flip the coin:
- what we see on this flip has no relation to previous or future flips
- the distribution of heads on every flip is identical. 

--

Our coin flip random process produces i.i.d. random variables. 

---



## Sample mean

Let's repeat our random sampling from the double coin flip, but we'll consider a smaller sample, of size $n = 100.$

```{r}
n <- 100
x_observed <- sample(X, prob = probs, replace = TRUE, size = n)

head(x_observed)
```

--

Our *sample mean* is the mean we observe in our data. 
--
This is one of the most commonly used sample statistics. It's called a plug-in estimator, because we just "plug in" the sample analog of the population quantity that we're interested in. 

--

$$\bar{X}_n = \frac{X_{1} + \dots + X_n}{n} = \frac{1}{n} \sum_{i = 1}^n X_i$$





--

```{r}
mean(x_observed)
```


---


We differentiate the *sample mean* from the *population mean* because the sample mean will vary with every new sample we draw. 

--

We'll use a simulation with `purrr::map` to see what would happen if we took a sample of size $n = 100$ from the population distribution many times. 

--

```{r}

n_iter <- 10000

x_list <- map(1:n_iter, ~ sample(X, prob = probs, replace = TRUE, 
                                 size = n))

head(x_list, 2)
```

---
```{r, message=FALSE}

sample_means <- map(x_list, mean)
head(sample_means)
```

---

```{r, message=FALSE}

df_sm <- tibble(sample_means = unlist(sample_means))
df_sm
```

---

```{r fig.width = 6, fig.height=4, fig.align='center'}
ggplot(df_sm, aes(x = sample_means)) +
  geom_histogram(bins = 20, position = 'identity', color = 'white') +
  geom_vline(xintercept = Ex, color = 'grey', lty = 'dashed')

```


--

We see the sample means are roughly distributed around the mean of the underlying population, `r Ex`. 

--

The expected value of the sample mean is the population mean. 


---

We can check that this is the case. 


$$
\begin{aligned}
\bar{X}\_n & = \frac{X\_{1} + \dots + X\_n}{n} = \frac{1}{n} \sum\_{i = 1}^n X\_i
\end{aligned}
$$

--


By linearity of expectations $\dots$

$$
\begin{aligned}
\textrm{E}[\bar{X}\_n] & = \frac{1}{n} \sum_{i = 1}^n \textrm{E}[X_i]\\
\end{aligned}
$$

--

$\dots$ because the $X_i$ are i.i.d. $\dots$

$$
\begin{aligned}
& = \frac{1}{n} \sum_{i = 1}^n \textrm{E}[X]
\end{aligned}
$$

--

$$
\begin{aligned}
& = \frac{1}{n} n \textrm{E}[X]\\
\end{aligned}
$$

--

$$
\begin{aligned}
& = \textrm{E}[X]
\end{aligned}
$$


--

*Refer to Aronow & Miller p.97 for more extended version of proof*. 

---

## Sample variance

We can estimate the mean of the population using the sample mean. What about the sample variance? 


---

## Sample variance

We'll do the same process with our simulations.

--

```{r fig.width = 6, fig.height=4, fig.align='center'}
sample_var <- map(x_list, var)
df_sv <- tibble(sample_var = unlist(sample_var))

ggplot(df_sv, aes(x = sample_var)) +
  geom_histogram(bins = 20, position = 'identity', color = 'white') +
  geom_vline(xintercept = sdx^2, color = 'grey', lty = 'dashed')
```


--

We see the sample variances are roughly distributed around the variance of the underlying population, `r sdx^2`. 

---


The formula for the unbiased sample variance is: 

$$S^2_n = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)^2$$
--

This looks a little bit different from a straightforward sample analog to the population variance formula, 

$$
\textrm{Var}[X] = \textrm{E}[(X-\textrm{E}[X]^2]
$$
--

Why do we divide by $n-1$, instead of $n$?

---

$$S^2_n = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)^2$$

The sample mean, $\bar{X}_n$, has an expected value of $\textrm{E}[X]$.

--

However, because it is made up of the $1, \dots, n$ $X_i$ that we actually observe, the expected difference between $(X_i - \bar{X}_n)$ is a little bit smaller than the expected difference between $(X_i - \textrm{E}[X])$. 


--

To account for this, we divide by $n-1$, instead of $n$. 

---
class: tiny

## 

We can check that the expected value of the unbiased sample variance estimator is the population variance. 
--
The estimator:

--

$$
\begin{aligned}
S^2\_n = \frac{1}{n-1}\sum\_{i = 1}^n (X\_i - \bar{X}\_n)^2
\end{aligned}
$$


--
Take the expectation:

$$
\begin{aligned}
\textrm{E}[S^2\_n] & = \textrm{E}\left[\frac{1}{n-1}\sum\_{i = 1}^n (X\_i - \bar{X}\_n)^2\right]
\end{aligned}
$$


--
By linearity of expectations:

$$
\begin{aligned}
 & = \frac{1}{n-1}\sum\_{i = 1}^n \textrm{E}\left[(X\_i - \bar{X}\_n)^2\right]
\end{aligned}
$$


--


$$
\begin{aligned}
& = \frac{1}{n-1}\sum\_{i= 1}^n \textrm{E}\left[X\_i^2 -2 X\_i\bar{X}\_n + \bar{X}\_n^2\right]
\end{aligned}
$$

---

<!-- -- -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -2\textrm{E}[ X\_i\frac{1}{n}\sum\_{j = 1}^n X\_j] + \textrm{E}\left[ \left(\frac{1}{n}\sum\_{j = 1}^n X_j \right) ^2\right]\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- --- -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -\frac{2}{n}\left(\textrm{E}[X\_i^2] +  \textrm{E}\left[\sum\_{j \neq i} X\_i X\_j\right]\right) + \textrm{E}\left[ \left(\frac{1}{n}\sum\_{j = 1}^n X_j \right) ^2\right]\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- -- -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -\frac{2}{n}\left(\textrm{E}[X\_i^2] +  \sum\_{j \neq i} \textrm{E}\left[ X\_i X\_j\right]\right) + \frac{1}{n^2} \textrm{E}\left[\sum\_{j = 1}^nX\_j^2 + \sum\_{j = 1}^n \sum\_{k \neq j}^nX\_j X\_k  \right]\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- --- -->

Expand expressions, and use linearity of expectations again. 

$$
\begin{aligned}
 & = \frac{1}{n-1}\sum\_{i= 1}^n \left( \textrm{E}[X\_i^2] -\frac{2}{n}\left(\textrm{E}[X\_i^2] +  \sum\_{j \neq i} \textrm{E}\left[ X\_i X\_j\right]\right) + \\\\
 \frac{1}{n^2} \left( \sum\_{j = 1}^n\textrm{E}[X\_j^2] + \sum\_{j = 1}^n \sum\_{k \neq j}^n \textrm{E}[ X\_j X\_k]  \right)\right)
\end{aligned}
$$

--

Because our data is i.i.d.:

$$
\begin{aligned}
 & = \frac{n}{n-1} \left( \textrm{E}[X^2] -\frac{2}{n}\left(\textrm{E}[X^2] +  (n-1) \textrm{E}[ X]^2\right) + \frac{1}{n^2} \left(n\textrm{E}[X^2] + n(n-1) \textrm{E}[ X]^2 \right) \right)
\end{aligned}
$$

<!-- -- -->



<!-- $$ -->
<!-- \begin{aligned} -->
<!--  & = \frac{n}{n-1} \left( \frac{n - 2 + 1}{n} \textrm{E}[X^2] + \frac{-2(n-1) + (n-1)}{n} \textrm{E}[ X]^2 \right) -->
<!-- \end{aligned} -->
<!-- $$ -->

--

Organizing terms:

$$
\begin{aligned}
 & = \frac{n}{n-1} \left( \frac{n - 1}{n} \textrm{E}[X^2] - \frac{n-1 }{n} \textrm{E}[ X]^2 \right)
\end{aligned}
$$

--

And simplifying: 

$$
\begin{aligned}
 & = \textrm{E}[X^2] - \textrm{E}[ X]^2 
\end{aligned}
$$

---

We can check to see how R calculates in the `var()` function. 

--

```{r}
head(x_observed)
```

--

```{r}
var(x_observed)
```

--

```{r}

mean( (x_observed - mean(x_observed) )^2)*n/(n-1)

```

--

R uses the formula for the unbiased sample variance. 







---
## Standard error of the estimator

The sample mean is itself a random variable, and so it has its own mean and variance. The mean of the sample mean is the population mean. The variance of the sample mean is:

$$
\textrm{Var}[\bar{X}\_n] = \frac{\textrm{Var}[X]}{n}
$$

--

And the standard deviation of the sample mean is:

$$
\sqrt{\textrm{Var}[\bar{X}\_n]} = \sqrt{\frac{\textrm{Var}[X]}{n}}
$$

--

We often refer to the standard deviation of an estimator as the *standard error*. 

--

- The *standard error* describes the sampling variation of an **estimator**; i.e., how much our estimates will vary based on the random sample that we draw. 

--

- The *standard deviation* describes the underlying variation in the **population distribution**. 


---

Why is $\textrm{Var}[\bar{X}_n] = \frac{\textrm{Var}[X]}{n}$?

--

The estimator:
$$
\begin{aligned}
\bar{X}\_n = \frac 1 n \sum\_{i = 1}^n X\_i
\end{aligned}
$$

--

Taking the variance on both sides:

$$
\textrm{Var}[\bar{X}\_n] = \textrm{Var}\left[ \frac 1 n \sum\_{i = 1}^n X\_i\right]
$$


--

By the variance rules:

$$
\begin{aligned}
\textrm{Var}[\bar{X}\_n] = \frac{1}{n^2}  \sum\_{i = 1}^n \textrm{Var}\left[X\_i\right]
\end{aligned}
$$

---

Because our observations are i.i.d.:

$$
\begin{aligned}
\textrm{Var}[\bar{X}\_n] = \frac{1}{n^2}  \sum\_{i = 1}^n \textrm{Var}\left[X\right]
\end{aligned}
$$
--
$$
\begin{aligned}
\textrm{Var}[\bar{X}\_n] = \frac{\textrm{Var}\left[X\right]}{n}
\end{aligned}
$$

---

Let's check this in our simulation as well. We saw that mathematically, $\textrm{Var}[X]$ was 0.375. So
$$
\frac{\textrm{Var}[X]}{n} = \frac{0.375}{100} = 0.00375
$$

--

```{r}
var(unlist(sample_means))
```

--

It's not exactly what we calculated mathematically. 

---

In fact, from our `r n_iter` separate samples, we calculate `r n_iter` separate sample means. From the variation in these sample means, we again *estimate* the variance of the sample mean. 

--

But *this estimate is itself a random variable*, with, again, its own sampling distribution. We will get slightly different estimates of the sampling variance of the sample mean each time we take our `r n_iter` separate samples. 


---
# Weak Law of Large Numbers



As our sample size $n$ grows, we are increasingly likely to observe a sample mean $\bar X_n$ that is close to the mean of the distribution, $\textrm{E}[X]$.

--

Formally, 

If $X_1, \dots, X_n$ are i.i.d. random variables,  then 

$$
\bar{X}_n \overset{p}{\to} \textrm{E}[X].
$$

--
Convergence in probability, $\overset{p}{\to}$, here means that the probability that we measure a value of $\bar X_n$ that is any arbitrary distance away from $\textrm{E}[X]$ is decreasing with our sample size. 



---

## 

To see the WLLN in action, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE,}
n <- 500
x_list <- map(1:8, ~ sample(X, prob = probs, replace = TRUE, 
                                 size = n))

x_mat <- as_tibble(x_list, .name_repair = 'unique')

new_mat <- x_mat %>% 
  mutate(across(everything(), ~ cumsum(.x)/seq_along(.x)),
         n = 1:n) %>% 
  pivot_longer(cols = starts_with('..'),
               values_to = "Sample mean")

```

---

## 

To see the WLLN in action, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE, echo = FALSE}

ggplot(new_mat %>% filter(n < 26), aes(x = n, y = `Sample mean`, col = name)) +
  geom_hline(yintercept = Ex, color = 'darkgrey', lty = 'dashed') +
  geom_line() +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(0, 500))

```

---
## 

To see the WLLN in action, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE, echo = FALSE}

ggplot(new_mat %>% filter(n < 51), aes(x = n, y = `Sample mean`, col = name)) +
  geom_hline(yintercept = Ex, color = 'darkgrey', lty = 'dashed') +
  geom_line() +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(0, 500))

```

---
## 

To see the WLLN in action, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE, echo = FALSE}

ggplot(new_mat %>% filter(n < 101), aes(x = n, y = `Sample mean`, col = name)) +
  geom_hline(yintercept = Ex, color = 'darkgrey', lty = 'dashed') +
  geom_line() +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(0, 500))

```

---
## 

To see the WLLN in action, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE, echo = FALSE}

ggplot(new_mat %>% filter(n < 201), aes(x = n, y = `Sample mean`, col = name)) +
  geom_hline(yintercept = Ex, color = 'darkgrey', lty = 'dashed') +
  geom_line() +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(0, 500))

```

---
## 

To see the WLLN in action, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE, echo = FALSE}

ggplot(new_mat %>% filter(n < 351), aes(x = n, y = `Sample mean`, col = name)) +
  geom_hline(yintercept = Ex, color = 'darkgrey', lty = 'dashed') +
  geom_line() +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(0, 500))

```

---

## 

To see the WLLN in action, we'll try simulating our coinflip process, but with an increasing number of samples used to calculate the sample mean. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', message=FALSE, echo = FALSE}

ggplot(new_mat %>% filter(n < 501), aes(x = n, y = `Sample mean`, col = name)) +
  geom_hline(yintercept = Ex, color = 'darkgrey', lty = 'dashed') +
  geom_line() +
  theme(legend.position = 'none') +
  coord_cartesian(xlim = c(0, 500))

```

---

Why is the WLLN so helpful to us?

--

Given a sufficient sample from a population, we can estimate features of a random variable to arbitrary precision

--

This is why we can use sample analogs of population features, like the sample mean, as plug-in estimators to estimate the population quantities.  

<!-- --- -->
<!-- ## Chebyshev? -->


---
# Bootstrap estimation

Another approach to estimating the standard error of an estimate is to use bootstrapping. 

--

- If we knew the CDF of our population, we would know exactly how to sample from the distribution to determine the sampling variation of our estimate. 

--

- While we do not, we can *suppose* that the empirical CDF produced by the data that we observe is identical to the population CDF. 

--

- We can then just resample with replacement from our observed data, and see how much our estimates vary across resamples. 

---


The bootstrapping procedure is:

- Repeat many times: 

    1. Take a sample of size $n$ *with replacement* from the observed data
--

    2. Apply the estimating procedure on the bootstrap sample. 

--

- Calculate the standard deviation of these many bootstrap estimates. 

---

Let's consider our coin flip example, with 100 observations. 

```{r bootstrap}
head(x_observed)
mean(x_observed)
var(x_observed)
sd(x_observed)

```

---

```{r}
n_boot <- 1000 # number of bootstrap iterations

boot_ests <- map(1:n_boot, # for n_boot number of times
                 # resample w/replacement
                 ~ sample(x_observed, replace = TRUE) %>%
                   mean()) # and calculate the resampled mean

head(boot_ests)
```

---

```{r}

sd(unlist(boot_ests))
```


--

Recall that the standard error of the mean is $\sqrt{\frac{\textrm{Var}[X]}{n}}$.

```{r}
sqrt(.375/100)
```


---

Bootstrapped estimates of the standard error are especially useful when our estimator is not as straightforward as the sample mean, for example estimators that involve ratios. 
--
Or when our sampling procedure is a bit more complicated, for example we sample "clusters" of units instead of individual units (we'll want to account for this in our bootstrap re-sampling).

--

It can be tricky to get a nice analytical solution for the standard error of the estimate in these situations, but the bootstrap estimator will tend to perform well when our data is i.i.d. 



---
# Central Limit Theorem

We can approximate probability statements about the sample mean, $\bar{X}_n$ using the Normal distribution. 

--

Formally, 

If $X_1, \dots, X_n$ are i.i.d. random variables with mean $\textrm{E}[X]$ and variance $\textrm{Var}[X]$, we can take a standardized version of the sample mean 

$$
Z_n \equiv  \frac{\bar{X}_n - \textrm{E}[X]}{\sqrt{\textrm{Var}[\bar{X}_n]}}
$$
which has mean 0 and variance 1. 
--
(Why?)

--

Then, 

$$
Z_n  \overset{d}{\rightarrow} \mathcal{N}(0,1)
$$


where $\mathcal{N}(0,1)$ indicates the Standard Normal Distribution with mean 0 and variance 1. 



---
class: tiny

## Normal distribution

The Normal distribution is frequently used in probability and statistics, because it is a useful approximation to many natural phenomena. 

$$
f(x) = \frac{1}{\sqrt{\sigma 2 \pi}}\textrm{exp}\left( -\frac{(x-\mu)^2}{2\sigma^2}\right) 
$$

It is defined by two parameters, $\mu$, the center of the distribution, and $\sigma$, which defines the distribution's standard deviation, or spread. The distribution is often notated $\mathcal{N}(\mu, \sigma^2)$.


---

It has a bell curve shape, with more density around the middle, and less density at more extreme values. 


```{r, fig.width = 6, fig.height=5, fig.align = 'center', echo=FALSE}
result_n <- rnorm(n = 10000)
plotdata <- tibble(
  x = result_n,
  Fx = pnorm(result_n),
  fx = dnorm(result_n)
)

g <- ggplot(plotdata, aes(x = x, y = fx)) +
  geom_line() +
  coord_cartesian(xlim = c(-2.5, 2.5),
                  ylim = c(0,0.5)) +
  geom_vline(xintercept = 0, lty = 'dashed', color = 'skyblue') +
  ggtitle('PDF of Standard Normal Distribution')

g + geom_segment(aes(x = 0, xend = -1, y = 0.2, yend = 0.2), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_segment(aes(x = 0, xend = 1, y = 0.2, yend = 0.2), 
               arrow = arrow(length = unit(0.25, "cm")), color = 'skyblue') +
  geom_point(aes(x = 0, y = 0.2), color = 'skyblue') + 
  annotate(geom="text", x = 0.5, y = .19, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = -0.5, y = .19, label = as.character(expression(sigma)), parse = TRUE, color = 'steelblue') + 
  annotate(geom="text", x = 0.075, y = .42, label = as.character(expression(mu)), parse = TRUE, color = 'steelblue')
```


---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }
n <- 1000
x_list <- map(1:10000, ~ sample(X, prob = probs, replace = TRUE, 
                                 size = n))

x_mat <- as_tibble(x_list, .name_repair = 'unique')

new_mat <- x_mat %>% 
  mutate(across(everything(), ~ (cumsum(.x)/seq_along(.x)-Ex)/(sqrt(0.375/row_number()) )) ,
         n = 1:n)# %>% 
  # pivot_longer(cols = starts_with('..'),
  #              values_to = "Sample mean")


x10 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:10]) )  
x50 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:50]) )  
x100 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:100]) )
x200 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:200]) )
x500 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:500]) )
x1000 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:1000]) )
x2000 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:2000]) )
x5000 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:5000]) )
x7500 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:7500]) )
x10000 <- tibble('Standardized sample mean' = unlist(new_mat[100,1:10000]) )

ggplot(x10, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('10 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x50, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('50 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---
To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x100, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('100 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x200, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('200 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x500, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('500 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x1000, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('1000 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x2000, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('2000 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x5000, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('5000 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---


To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x7500, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('7500 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

To see the CLT in action, we'll try simulating our coinflip process many times. We'll keep the individual sample size fixed at $n=$ 100. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }

ggplot(x10000, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02) + 
  ggtitle('10000 samples') +
  coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(0, 800))

```

---

We can rescale the y axis to get relative frequency of sample means, and overlay the normal PDF. 


```{r,fig.width = 5, fig.height=5, fig.align = 'center', echo = FALSE, message=FALSE }


ggplot(x10000, aes(x = `Standardized sample mean`)) +
  geom_histogram(binwidth = .02, aes(y = ..count../1500)) + 
  ggtitle('10000 samples') +
  ylab('Relative frequency') +
  coord_cartesian(xlim = c(-2.5, 2.5)) +
  stat_function(fun=dnorm,
                         color="red",
                         args=list(mean=0,
                                  sd=1))

```

---
Why is the Central Limit Theorem so helpful to us?

--

It gives us a structure for quantifying our uncertainty under sufficiently large samples. 


---
# Beyond i.i.d.

- In practice, beyond simple processes like coin flips and dice, it's hard to defend the claim that our data is *literally* i.i.d.

--

- The i.i.d. assumption is an approximation that gives us leverage to make statistical claims. Basically, we'd like to think that our data is close-enough to i.i.d. 

---
# Finite population random sampling

Suppose we are sampling from some finite population of units; say, students in a classroom, and we will measure ages. Suppose there are 5 students, with ages 7, 7, 7, 8, 8, and 9. We'll consider a random variable $X$, which is just one draw from the class. 

--

We can identify the target mean and variance from our small population. 

--

- $X = \{7,8,9\}$
- $\textrm{E}[X] = 7 \frac 2 3$
- $\textrm{E}[X] = \textrm{E}[X^2]- \textrm{E}[X]^2 = \frac 5 9$

--

We can randomly sample from the class *with replacement*, i.e., after each draw, we can resample from all students in the class again: each draw is independent and identically distributed.

--

If we randomly sample *without replacement*, draws are not independent--if I draw the student who is age 9 on my first draw, I can't get another age 9 draw again, and I'll run out after 5 draws. 


---

In practice most of our sampling in the social sciences is finite population random sampling *without* replacement. 

--

This is a problem in my tiny classroom. But if the sample is sufficiently large, removing a few units doesn't affect the distribution for remaining draws in a meaningful way. 

--

Suppose I am sampling from *all* third graders in the US. We can define the mean and variance for this population; suppose there are 1.2 million third graders age 7, 1 million third graders 8, and 1.4 million third graders age 9. The mean in this population is 8.056 and variance is 0.809. 

--

After I sample a few third graders, the mean and variance for the remaining students is still approximately the same. 

--

When we're dealing with sampling from finite populations, we often make the assumption that we're sampling from a hypothetical, infinite *superpopulation* that is so large that there is no effect of sampling without replacement. 

<!-- --- -->

<!-- # De moivre la place theorem -->

<!-- --- -->

<!--  - consistency -->
<!--  - mse -->

<!--  - heteroskedasticity -->
<!--  cluster robust -->
