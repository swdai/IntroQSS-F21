---
title: 3.1 Brief Overview of Probability
subtitle: PLSC30500, Fall 2021
author: 
  - co-taught by Molly Offer-Westort & Andy Eggers
  - .small-text[(This lecture with references to Aronow & Miller (2019) and Wasserman (2004))]
output:
  xaringan::moon_reader:
    css: [default, robot, robot-fonts]
    nature:
      highlightStyle: atelier-lakeside-light
      highlightLines: yes
      countIncrementalSlides: no
    pandoc_args: --metadata-file=./common.yaml
---
```{r setup, include=FALSE}
library(tidyverse)
set.seed(60637)
```

```{css, echo=FALSE}
.small-output .remark-code{
  font-size: x-small;
}

.small-text {
  font-size: small;
}
```

## What do we do with data

Now that we've gotten started on working with data...what do we want to get from that data?

--

- Describe what's going on in the data
 
--

 - We can do a pretty good job of this with the data visualization tools we have, along with summary statistics for numerical descriptions

--

This is already a really useful start, but beyond just describing the data we see in front of us, we may have other goals. We may want to:

--

- Make generalizations to a larger population

--

- Make informed guesses about how things would have turned out differently, if something different had happened

--

These latter two are called *inference*, and we'll need some additional tools and assumptions to make headway on them

---

# Why we use probability theory

- Probability theory allows us to talk about *random* events in structured way. 

--

 - Often, we see only part of the picture we'd like to talk about. We only see some of the data, or we can only see one version of events. 

--

 - Probability theory gives us a way to describe the process that results in the data that we observe. 

--

 - It also allows us to *formalize our uncertainty* when making inference. 


---

## What does it mean to describe real world events as "random"?

- Probability theory is an abstract construct, but it is useful for empirical research to create a model of the world in which events are probabilistic. 

--

 - If we are conducting measurement on some population but can only observe a sample, we assume that there is some randomness to who we observe and who we don't observe.

--


 - If we are describing events with counterfactuals, such as turning out to vote or testing positive for COVID, it can be useful to describe those events as probabilistic.

--


- These are *models* of how the world works, and they help us make sense of the fundamentally squishy nature of social science research. With limited information about the world, we operate with uncertainty. Assigning probabilities conditional on the information we *do* have helps us formalize that uncertainty. 
--
Even if we don't necessarily believe that human behavior is "random." 

---
## Flipping a coin twice

Suppose we are flipping a coin twice, and the coin is fair. This is a random process, and we will describe the probability space associated with this process. 


---


name: terms

## Useful terms

<small>
--

-  $\Omega$ : Sample space. Describes all possible outcomes in our setting.

--

 - $\omega$ : Generic notation for the realized outcomes in the sample space.[1]
 
  .footnote[[1]In probability, we often use uppercase terms to denote the random process, and lowercase terms to denote specific outcomes of that random process.]

--

 - Here, $\Omega = \{HH, HT, TH, TT \}$.

--

- Event: a subset of $\Omega$. 
--

 - We will often use terms like $A$ or $B$ to define events. 
 
--
 
 - In our example, the event that we get a head on the first flip is $A = \{HT, HH\}$. 

--

- $S$ : Event space. Describes all subsets of events, including the null set. [[Full event space]](#event_space)

--

 - We use this in addition to the sample space, so we can describe all types of events that we can define the probability for. 

--

- $P$ : Probability measure. A function that assigns probability to all of the events in the event space. 

--

 - Here, since our coin is fair, for the event that we get a head on the first flip, $P(A) = 1/2$. 


</small>

---
## An aside on mathematical notation 

Mathematical notation is a tool that gives us a common language to express concepts with precision. There are different conventions in different communities, and no approach is "right" or "wrong"--it's just a question of whether your notation is appropriately communicating to your audience what you want it to. 

--

To use mathematical notation in R Markdown, write LaTeX typesetting commands inside of dollar signs. 
--


For example, 

`$Y = \beta_0 + \beta_1 X$` 

--

is rendered as 

$Y = \beta_0 + \beta_1 X$.


We have just used probability notation as a way to fully describe any random generative process. 

---

We can simulate our double coinflip process in R as well, using the `sample()` function. 

--

```{r coinflip}
omega <- c('HH', 'HT', 'TH', 'TT')
probs <- c(0.25, 0.25, 0.25, 0.25)

sample(x = omega,
       size = 1,
       prob = probs)

```

--

We can run this simulation many times, and our results should *approximately* follow the probabilities we assigned. 

```{r coinflip_many}
n <- 1000
result_n <- sample(x = omega,
       size = n,
       prob = probs,
       replace = TRUE)

table(result_n)
```


---
## Random variables

- A random variable is a mapping $X$ from our sample space $\Omega$, to the Real numbers. 
$$X : \Omega \to {\rm I\!R}$$

- Random variables are ways to quantify random events described by our sample space. 
- We'll mostly work with random variables going forward, but it's important to remember that the random variable is built on the foundations of the sample space -- and often, you'll be the one deciding how that quantification happens. 

---

For example, with our two coin flips, let $X(\omega)$ be the number of heads in the sequence $\omega$. Then the random variable, and its probability distribution, can be described as:


|     $\omega$ | $P(\{\omega\})$ | $X(\omega)$ |
|-------------:|----------------:|------------:|
| TT           | 1/4             | 0           |
| TH           | 1/4             | 1           |
| HT           | 1/4             | 1           |
| HH           | 1/4             | 2           |

and,

| $x$   | $P(X = x)$
|----:|------------:|
| 0   | 1/4         |
| 1   | 1/2         |
| 2   | 1/4         |

---

We can simulate this in `R` as well. 

```{r coinflipRV}
X <- c(0, 1, 2)
probs <- c(0.25, 0.5, 0.25)

sample(x = X,
       size = 1,
       prob = probs)

```

```{r coinflip_manyRV}
result_n <- sample(x = X,
       size = n,
       prob = probs,
       replace = TRUE)

table(result_n)
```

```{r coinflip_massRV}

prop.table(table(result_n))

```

---

We can plot a histogram to look at the distribution of results. 
```{r fig.width = 6, fig.height=6, fig.align='center'}

ggplot(tibble(result_n), aes(x = result_n)) +
  geom_histogram(bins = 3, position = 'identity', color = 'white')

```

---

## Probability Mass Function of a discrete random variable

- A random variable is *discrete* if it takes countably many values. 
- The probability mass function of a discrete RV $X$ tells us the probability we will see an outcome at some value $x$. 

$$
f(x) = P(X = x)
$$
For our coin flip example, 


$$
f(x) = \begin{cases}
1/4 & x = 0 \\\
1/2 & x = 1 \\\
1/4 & x = 2 \\\
0 & \text{otherwise}
\end{cases}
$$

---

## Illustrating the PMF of a discrete RV

```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}
plotdata <- tibble(
  x = c(-1, 0, 1, 2),
  xend = c(0, 1, 2, 3),
  Fx = c(0, 1/4, 3/4, 1),
  fx = c(0, 1/4, 1/2, 1/4)
)

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_point() +
  coord_cartesian(xlim = c(-0.5, 2.5),
                  ylim = c(0,1)) +
  ggtitle('PMF of X as number of heads in 2 fair coin flips')
```

---
## Cumulative Distribution Functions

- The cumulative distribution function of $X$ tells us the probability we will see an outcome less than or equal to some value $x$. 

$$
F(x) = P(X \le x)
$$


For our coin flip example, 


$$
F(x) = \begin{cases}
0 & x < 0 \\\
1/4 & 0 \le x < 1 \\\
3/4 & 1 \le x < 2 \\\
1 & x \ge 2
\end{cases}
$$

---

## Illustrating the CDF of a discrete RV

```{r coinflip_plotRV, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}
ggplot(plotdata, aes(x = x, y = Fx)) +
  geom_segment(aes(x = x, y = Fx, xend = xend, yend = Fx)) + 
  geom_point() +
  geom_point(aes(x = xend, y = Fx), shape= 21, fill = 'white') +
  coord_cartesian(xlim = c(-0.5, 2.5),
                  ylim = c(0,1)) +
  ggtitle('CDF of X as number of heads in 2 fair coin flips')
```

---
And we can use `ggplot2` to see what the *Empirical* CDF looks like
```{r coinflip_simRV, fig.width = 5, fig.height=5, fig.align = 'center'}

ggplot(tibble(result_n), aes(x = result_n)) +
  stat_ecdf() +
  coord_cartesian(xlim = c(-0.5, 2.5)) +
  ylab('Empirical Fx') +
  ggtitle('ECDF of X as number of heads in 2 fair coin flips')

```

---

## Continuous random variables

- So far, our coin flip example was for a *discrete* random variable.  
- A random variable is *continuous* if it has a continuous density function
 - Practically, we will treat RVs as discrete if they have countably many outcomes, and RVs as continuous if the number of values they can take on is only constrained by our measurement tool.

---

## Uniform distribution

- If you take a draw from the standard uniform distribution, you are equally likely to draw any number between zero and one.

- We can simulate this in R. R allows you to sample from a number of canonical distributions; to see which distributions are available, search `?Distributions`. 

```{r standard_uniform}
runif(n = 1, min = 0, max = 1)
```

---

We can again sample from the distribution many times, and plot a histogram to look at the distribution of results. 

```{r standard_uniform_many, fig.width = 6, fig.height=4, fig.align = 'center'}
result_n <- runif(n, min = 0, max = 1)

ggplot(tibble(result_n), aes(x = result_n)) +
  geom_histogram(breaks = seq(0, 1, length.out = 15),  
                 position = 'identity', color = 'white')
```

---

## Probability Density Function of continuous random variables



- Discrete random variables have non-zero mass on specific points, but for continuous random variables, $P(X = x) = 0$. Instead of mass, we refer to *density* for continuous variables. [2]



.footnote[[2] Measure theory give a unified approach to measuring discrete and continuous random variables, but for simplicity, we'll keep the dichotomy of mass vs. density for discrete/continuous here. ]

- The *probability density function* $f(x)$ for a continuous random variable gives the slope of the CDF at any given point. This means that we can integrate the area under the PDF to get the relative probability of being between two points. 

$$
P(a < X < b) = \int_a^b f(x)dx
$$

---

## Illustrating the CDF of a continuous RV
<small>
- We start by showing the CDF of the standard uniform distribution, to illustrate how the PDF relates to the CDF. 

- The CDF for the standard uniform distribution is:

$$
F(x) = 
\begin{cases}
0 & x < 0\\\
x & 0 \le x \le 1 \\\
1 & x > 1
\end{cases}
$$

- Notice that the slope is 1 between 0 and 1. 
</small>

```{r uniform_plot, fig.width = 4, fig.height=4, fig.align = 'center', echo=FALSE}


plotdata <- tibble(
  x = c(-1, 0, 1, 2),
  Fx = c(0, 0, 1, 1)
)

ggplot(plotdata, aes(x = x, y = Fx)) +
  geom_line() + 
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  ggtitle('CDF of Standard Uniform Distribution')
```

---

## Illustrating the PDF of a continuous random variable

- The PDF for the standard uniform distribution is:
$$
f(x) = \begin{cases}
1 & 0 < x < 1\\\
0 & \text{otherwise.}
\end{cases}
$$

```{r, fig.width = 4, fig.height=4, fig.align = 'center', echo=FALSE}
plotdata <- tibble(
  x = c(-1, 0, 1, 1),
  xend = c(0, 1, 1, 2),
  fx = c(0, 1, 1, 0)
)

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_segment(aes(x = x, y = fx, xend = xend, yend = fx)) + 
  geom_point() +
  geom_point(aes(x = 0, y = 0), shape= 21, fill = 'white') +
  geom_point(aes(x = 1, y = 0), shape= 21, fill = 'white') +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  ggtitle('PDF of Standard Uniform Distribution')
```

---

- Notice that if we take the area under the density curve, the total area will sum to 1.  


```{r, fig.width = 4, fig.height=4, fig.align = 'center', echo=FALSE}
datapoly <- tibble(x = c(0, 0, 1, 1),
                   y = c(0, 1, 1, 0))

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_segment(aes(x = x, y = fx, xend = xend, yend = fx)) + 
  geom_point() +
  geom_point(aes(x = 0, y = 0), shape= 21, fill = 'white') +
  geom_point(aes(x = 1, y = 0), shape= 21, fill = 'white') +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  ggtitle('PDF of Standard Uniform Distribution')
  
```

---

- Notice that if we take the area under the density curve, the total area will sum to 1. Relative density gives us relative probability. 


```{r, fig.width = 4, fig.height=4, fig.align = 'center', echo=FALSE}
datapoly <- tibble(x = c(0, 0, 1, 1),
                   y = c(0, 1, 1, 0))

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_segment(aes(x = x, y = fx, xend = xend, yend = fx)) + 
  geom_point() +
  geom_point(aes(x = 0, y = 0), shape= 21, fill = 'white') +
  geom_point(aes(x = 1, y = 0), shape= 21, fill = 'white') +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  geom_polygon(data = datapoly, aes(x = x, y = y), fill = 'blue', alpha = 0.5) + 
  ggtitle('PDF of Standard Uniform Distribution')
```

---

- If we want to get the probability $X$ is between 0 and 0.75, 

$$
P(0 \le x \le .75) = \int_0^.75 f(x)dx
$$
--
we take the area under the density curve between 0 and 0.75 -- which is also 0.75. (Notice that we don't need to use calculus here.) 


```{r, fig.width = 5, fig.height=5, fig.align = 'center', echo=FALSE}
datapoly <- tibble(x = c(0, 0, 0.75, 0.75),
                   y = c(0, 1, 1, 0))

ggplot(plotdata, aes(x = x, y = fx)) +
  geom_segment(aes(x = x, y = fx, xend = xend, yend = fx)) + 
  geom_point() +
  geom_point(aes(x = 0, y = 0), shape= 21, fill = 'white') +
  geom_point(aes(x = 1, y = 0), shape= 21, fill = 'white') +
  coord_cartesian(xlim = c(-0.5, 1.5),
                  ylim = c(0,1)) +
  geom_polygon(data = datapoly, aes(x = x, y = y), fill = 'blue', alpha = 0.5) + 
  ggtitle('PDF of Standard Uniform Distribution')
```




---
## Some reference texts

1. Aronow, P., & Miller, B. (2019). Foundations of Agnostic Statistics. Cambridge: Cambridge University Press. Chapter 1. Probability Theory. 
1. Wasserman, L. (2004). All of Statistics: A Concise Course in Statistical Inference. New York: Springer. Chapter 1. Probability Theory and Chapter 2. Random Variables. 
1. HernÃ¡n MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.

---

name: event_space

Flipping two coins event space:


$$\begin{align}
S & = \{\emptyset, \\
&\{HH\}, \{HT\},\{TH\}, \{TT\},\\
&\{HH, HT\}, \{HH, TH\},\{HH, TT\}, \{HT, TH\}, \{HT, TT\}, \{TH, TT\}\\
& \{HH, HT, TH\}, \{HH, HT, TT\}, \{HH TH, TT\}, \{HT, TH, TT\},\\
&\{HH, HT, TH, TT\} \}
\end{align}$$

[[Back to terms]](#terms)
